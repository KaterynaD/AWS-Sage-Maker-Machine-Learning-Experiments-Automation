{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Sagemaker XGBoost Batch Transform with ShapValues\n",
    "Purpose: generate predictions along with ShapValues\n",
    "Main Steps: \n",
    "1. Create input files in S3 in a format suitable for AWS Sagemaker Batch Transform job\n",
    "2. Run Batch Transform job and create a file in S3 with predictions and Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder='/home/kate/Research/YearBuilt/Notebooks/Experiments_v2/tmp/'\n",
    "Experiments_file='/home/kate/Research/YearBuilt/Experiments/DevExperiments.xlsx'\n",
    "AllExperiments_tab='Experiments'\n",
    "Experiment_name='prediction'\n",
    "#Experiment configuration: differenet datasets to predict from\n",
    "#1.each line in the file contains the model name and set of features to built a dataset for SageMaker with only specific features\n",
    "Experiment_tab='%s Models'%Experiment_name\n",
    "#2.ModelFiles: each line is a model name (Model) and full model file name (ModelData - model.tar.gz) in an S3 bucket. SageMaker models will be created based on the data\n",
    "Experiment_ModelFiles_tab='%s ModelFiles'%Experiment_name\n",
    "\n",
    "Trial_name_preprocessing='%s-PreparingData'%Experiment_name\n",
    "Trial_name_inference='%s-Inference'%Experiment_name\n",
    "\n",
    "\n",
    "\n",
    "bucket='kdproperty'\n",
    "path_to_data='Data'\n",
    "path_to_input_data='Data/Experiments/%s/'%Experiment_name\n",
    "path_to_output_data='Data/Experiments/%s/Prediction/'%Experiment_name\n",
    "path_to_configuration='Config'\n",
    "\n",
    "\n",
    "instance_type_inference='ml.m5.xlarge'\n",
    "#input data files can be splitted to parts and the number of instances should be proportinal to speed up the process\n",
    "instance_count_inference=5 \n",
    "\n",
    "\n",
    "instance_type_preprocessing='ml.t3.2xlarge'\n",
    "instance_count_preprocessing=1\n",
    "\n",
    "#timeout for waiting Shap Values\n",
    "transformation_job_timeout = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "from sagemaker.xgboost.model import XGBoostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role = 'arn:aws:iam::XYZ:role/service-role/AmazonSageMaker-ExecutionRole-20200819T131882'\n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append('/home/kate/Research/YearBuilt/Notebooks/Experiments')\n",
    "import ExperimentsUtils as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=AllExperiments_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading experiment configuration from an excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Target variable and data file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target of models in prediction experiment is hasclaim_water\n",
      "Datafile used in prediction experiment is dwelling_basedata_v4.csv\n"
     ]
    }
   ],
   "source": [
    "target=experiments[experiments['Experiment']==Experiment_name]['Target'].values[0]\n",
    "print('Target of models in %s experiment is %s'%(Experiment_name,target))\n",
    "data_file=experiments[experiments['Experiment']==Experiment_name]['Dataset'].values[0]\n",
    "print('Datafile used in %s experiment is %s'%(Experiment_name,data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Features to create datafiles in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PropertyAgeFold0</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model                  F1               F2    F3  \\\n",
       "0  PropertyAgeFold0  cal_year-yearbuilt  cova_deductible  sqft   \n",
       "\n",
       "                 F4  \n",
       "0  water_risk_3_blk  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all_features = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_tab)\n",
    "model_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Model files (usually model.tar.gz produced from training)\n",
    "Later SageMaker Models will be created  based on this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ModelData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PropertyAgeFold0</td>\n",
       "      <td>s3://kdproperty/Models/Experiments/bf2/Propert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model                                          ModelData\n",
       "0  PropertyAgeFold0  s3://kdproperty/Models/Experiments/bf2/Propert..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_files = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_ModelFiles_tab)\n",
    "model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.Verification if we have the same set of  models in both configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_from_model_features=model_all_features['Model'].tolist()\n",
    "models_from_models_files=model_files['Model'].tolist()\n",
    "if len([x for x in models_from_model_features if x not in models_from_models_files])!=0:\n",
    "    raise Exception('Different set of models in featuresets and files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Saving into S3 models configurations (sets of features) to be used in data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Config_file='%s.csv'%Experiment_name\n",
    "Models_Config_path = os.path.join(temp_folder, Model_Config_file) \n",
    "\n",
    "model_all_features.to_csv(Models_Config_path, header=True, index=False)\n",
    "\n",
    "\n",
    "input_code = sagemaker_session.upload_data(\n",
    "        Models_Config_path,\n",
    "        bucket=bucket,\n",
    "        key_prefix=path_to_configuration\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Creating experiments and trials in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "eu.cleanup_experiment(Experiment_name)\n",
    "eu.create_experiment(Experiment_name)\n",
    "eu.create_trial(Experiment_name,Trial_name_preprocessing)\n",
    "eu.create_trial(Experiment_name,Trial_name_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preparing datasets for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Script to create a datafile in a Processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessingDataForPrediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessingDataForPrediction.py\n",
    "\n",
    "#no headers\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_file', type=str)\n",
    "    parser.add_argument('--config_file', type=str) \n",
    "    parser.add_argument('--split_to_N_parts', type=int, default=1)\n",
    "    args, _ = parser.parse_known_args()    \n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "    split_to_N_parts=args.split_to_N_parts\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', args.data_file)\n",
    "    config_data_path = os.path.join('/opt/ml/processing/config', args.config_file)\n",
    "\n",
    "\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    dataset = pd.read_csv(input_data_path, error_bad_lines=False, index_col=False)\n",
    "    \n",
    "\n",
    "    print('Reading config data from {}'.format(config_data_path))\n",
    "    models = pd.read_csv(config_data_path, error_bad_lines=False, index_col=False)      \n",
    "\n",
    "  \n",
    "    #iterating thru config file with models and featureset\n",
    "    for index, row in models.iterrows():\n",
    "        model=row['Model']\n",
    "        print (index, ': Creating featuresets for model %s'%model)\n",
    "        featureset=row[1:51].tolist()\n",
    "        featureset=[x for x in featureset if str(x) != 'nan']    \n",
    "\n",
    "        X = pd.DataFrame()\n",
    "        for f in featureset:\n",
    "            X[f]=dataset.eval(f)        \n",
    "        \n",
    "        if not os.path.exists('/opt/ml/processing/output/%s'%model):\n",
    "            os.makedirs('/opt/ml/processing/output/%s'%model)\n",
    "        output_data_path = os.path.join('/opt/ml/processing/output/%s'%model, 'data.csv')\n",
    "        if split_to_N_parts>1:\n",
    "            parts = np.array_split(X, split_to_N_parts)\n",
    "            for i,p in enumerate(parts):\n",
    "                output_data_path = os.path.join('/opt/ml/processing/output/%s'%model, 'data_%s.csv'%i)\n",
    "                p.to_csv(output_data_path,header=False,index=False)\n",
    "        else:           \n",
    "            X.to_csv(output_data_path, header=False, index=False)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Processors and waiting job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2021-02-28-19-13-33-728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-02-28-19-13-33-728\n",
      "Inputs:  [{'InputName': 'data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://kdproperty/Data/dwelling_basedata_v4.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'config', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://kdproperty/Config/prediction.csv', 'LocalPath': '/opt/ml/processing/config', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-XYZ/sagemaker-scikit-learn-2021-02-28-19-13-33-728/input/code/preprocessingDataForPrediction.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://kdproperty/Data/Experiments/prediction/', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34mReceived arguments Namespace(config_file='prediction.csv', data_file='dwelling_basedata_v4.csv', split_to_N_parts=5)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/dwelling_basedata_v4.csv\u001b[0m\n",
      "\u001b[34mReading config data from /opt/ml/processing/config/prediction.csv\u001b[0m\n",
      "\u001b[34m0 : Creating featuresets for model PropertyAgeFold0\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processors=list()\n",
    "\n",
    "\n",
    "\n",
    "data_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type=instance_type_preprocessing,\n",
    "                                     instance_count=instance_count_preprocessing)\n",
    "    \n",
    "data_processor.run(code='preprocessingDataForPrediction.py',\n",
    "                        inputs= [ProcessingInput(input_name='data',source='s3://%s/%s/%s'%(bucket,path_to_data,data_file),destination='/opt/ml/processing/input'),\n",
    "                                ProcessingInput(input_name='config',source='s3://%s/%s/%s'%(bucket,path_to_configuration,Model_Config_file),destination='/opt/ml/processing/config'),\n",
    "                                ],\n",
    "                        outputs=[\n",
    "                                ProcessingOutput(output_name='output', source='/opt/ml/processing/output', destination='s3://%s/%s'%(bucket,path_to_input_data)),                                                          \n",
    "                                ],\n",
    "                        arguments=['--data_file',data_file,\n",
    "                                '--config_file',Model_Config_file,\n",
    "                                 '--split_to_N_parts',str(instance_count_inference)],\n",
    "                        experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name_preprocessing,\n",
    "        'TrialComponentDisplayName' : Trial_name_preprocessing},\n",
    "                        wait=True\n",
    "                        )\n",
    "processors.append(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the execution if there is an issue with creating input data for the models\n",
    "job_name=data_processor.jobs[-1].describe()['ProcessingJobName']\n",
    "if not(sagemaker_session.was_processing_job_successful(job_name)):\n",
    "    raise Exception('Preprocessing job Failed!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Running inference jobs to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Script for inference with Shap Values. The file must have name inference.py!!!!\n",
    "See the very last lines of the code below how Shap values are produced and returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker_xgboost_container.encoder as xgb_encoders\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialize and return fitted model.\n",
    "    \"\"\"\n",
    "    model_file = \"xgboost-model\"\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), \"rb\"))\n",
    "    return booster\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    The SageMaker XGBoost model server receives the request data body and the content type,\n",
    "    and invokes the `input_fn`.\n",
    "\n",
    "    Return a DMatrix (an object that can be passed to predict_fn).\n",
    "    \"\"\"\n",
    "    if request_content_type == \"text/csv\":\n",
    "        return xgb_encoders.csv_to_dmatrix(request_body.rstrip('\\n').lstrip('\\n'))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Content type {} is not supported.\".format(request_content_type)\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    SageMaker XGBoost model server invokes `predict_fn` on the return value of `input_fn`.\n",
    "\n",
    "    Return a two-dimensional NumPy array where the first columns are predictions\n",
    "    and the remaining columns are the feature contributions (SHAP values) for that prediction.\n",
    "    \"\"\"\n",
    "    prediction = model.predict(input_data)\n",
    "    feature_contribs = model.predict(input_data, pred_contribs=True)\n",
    "    output = np.hstack((prediction[:, np.newaxis], feature_contribs))\n",
    "    \n",
    "    return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Creating models in SageMaker to be used in interference(prediction) based on model files provided in models_ModelFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropertyAgeFold0 s3://kdproperty/Models/Experiments/bf2/PropertyAge-1-2021-02-16-16-37-37/output/model.tar.gz\n",
      "PropertyAgeFold0 model does not exist\n",
      "PropertyAgeFold0 model was created\n"
     ]
    }
   ],
   "source": [
    "models = list()\n",
    "model_names = list()\n",
    "i = 0\n",
    "for index, row in model_files.iterrows():\n",
    "    #Try to delete if exists model and create a new model based on a model file\n",
    "    name=row['Model']\n",
    "    name=name.replace('_','-')\n",
    "    model_data=row['ModelData']\n",
    "    print(name,model_data)\n",
    "    try:\n",
    "        response = smclient.delete_model(ModelName=name)\n",
    "        print('%s model was deleted'%name)\n",
    "    except:\n",
    "        print('%s model does not exist'%name)\n",
    "        pass\n",
    "    xgb_inference_model = XGBoostModel(\n",
    "    name=name,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version=\"1.0-1\",\n",
    "    )\n",
    "    models.append(xgb_inference_model)\n",
    "    model_names.append(name)\n",
    "    print('%s model was created'%name)\n",
    "    i = i + 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Running transform jobs using inference.py script and models created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropertyAgeFold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: PropertyAgeFold0\n",
      "WARNING:sagemaker:Using already existing model: PropertyAgeFold0\n",
      "INFO:sagemaker:Creating transform job with name: PropertyAgeFold0-2021-02-28-19-18-25-542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job PropertyAgeFold0-2021-02-28-19-18-25-542 started\n"
     ]
    }
   ],
   "source": [
    "tranform_jobs = list()\n",
    "tranformers = list()\n",
    "i = 0\n",
    "for m,model_name in zip(models,model_names):   \n",
    "    s3_batch_input='s3://%s/%s%s'%(bucket,path_to_input_data,model_name)\n",
    "    s3_batch_output_model = 's3://%s/%s%s'%(bucket,path_to_output_data,model_name)\n",
    "    print(model_name)\n",
    "    transformer =  m.transformer(\n",
    "                                              instance_count=instance_count_inference, \n",
    "                                              instance_type=instance_type_inference,\n",
    "                                              output_path=s3_batch_output_model,\n",
    "                                              accept='text/csv',\n",
    "                                              strategy='MultiRecord',\n",
    "                                              assemble_with='Line',\n",
    "                                              env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : str(transformation_job_timeout)}\n",
    "                                )\n",
    "    tranformers.append(transformer)\n",
    "    transformer.transform(data=s3_batch_input, content_type='text/csv',split_type='Line', wait=False,\n",
    "    experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name_inference,\n",
    "        'TrialComponentDisplayName' : '%s-%s'%(Trial_name_inference,model_name.replace('_','-')),})\n",
    "    job_name = transformer.latest_transform_job.name\n",
    "    tranform_jobs.append(job_name)\n",
    "    print('Job %s started'%job_name)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming job PropertyAgeFold0-2021-02-28-19-18-25-542 status: InProgress\n",
      "Continue waiting...\n",
      "All Transforming Jobs are Completed\n"
     ]
    }
   ],
   "source": [
    "eu.wait_transform_jobs(processors=tranformers,tranform_jobs=tranform_jobs,check_every_sec=10,print_every_n_output=20,wait_min=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The resulting files are huge and the best next step is to directly load them in Redshift for analyzing and visualizing in a BI tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
