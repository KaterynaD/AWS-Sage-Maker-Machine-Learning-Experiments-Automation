{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiments_file='/home/kate/Research/YearBuilt/Experiments/DwellingExperiments.xlsx'\n",
    "AllExperiments_tab='Experiments'\n",
    "Experiment_name='FWaterClaims'\n",
    "#Experiment configuration: differenet datasets to try\n",
    "#each line in the file contains the model name and set of features to built a dataset for SageMaker\n",
    "Experiment_tab='%s Models'%Experiment_name\n",
    "\n",
    "#Looks like Trial name should be unique in my environment, not in the experiment it belongs to\n",
    "Trial_name='%s-PreparingTrainValidData'%Experiment_name\n",
    "\n",
    "\n",
    "\n",
    "#original dataset was created from a Redshift query and uploaded into S3\n",
    "bucket='kdproperty'\n",
    "path_to_data_file='/Data/'\n",
    "\n",
    "\n",
    "split_year='2020'\n",
    "val_size='0.25'\n",
    "\n",
    "\n",
    "instance_type='ml.t3.large'\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/kate/Research/YearBuilt/Notebooks/Property')\n",
    "import ExperimentsUtils as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FWaterClaims is a new experiment. nothing to delete\n"
     ]
    }
   ],
   "source": [
    "#delete experiment and trials but not output files associated with jobs in experiments\n",
    "eu.cleanup_experiment(Experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "eu.create_experiment(Experiment_name)\n",
    "eu.create_trial(Experiment_name,Trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasclaim_water\n",
      "dwelling_basedata_v4.csv\n"
     ]
    }
   ],
   "source": [
    "experiments = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=AllExperiments_tab)\n",
    "target=experiments[experiments['Experiment']==Experiment_name]['Target'].values[0]\n",
    "print(target)\n",
    "data_file=experiments[experiments['Experiment']==Experiment_name]['Dataset'].values[0]\n",
    "print(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FWaterClaims</td>\n",
       "      <td>usagetype_encd</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>cova_limit</td>\n",
       "      <td>sqft</td>\n",
       "      <td>cal_year - yearbuilt</td>\n",
       "      <td>landlordind</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "      <td>constructioncd_encd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model              F1               F2           F3    F4  \\\n",
       "0  FWaterClaims  usagetype_encd  cova_deductible  cova_limit   sqft   \n",
       "\n",
       "                     F5           F6                F7                   F8  \n",
       "0  cal_year - yearbuilt  landlordind  water_risk_3_blk  constructioncd_encd  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_tab)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role = 'arn:aws:iam::757107622481:role/service-role/AmazonSageMaker-ExecutionRole-20200819T131882'\n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "#Training and Validation dataset for SageMaker are the same structure: no headers, the first column is a target and the rest are features\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_file', type=str)\n",
    "    parser.add_argument('--split_year', type=int)     \n",
    "    parser.add_argument('--val_size', type=float)     \n",
    "    parser.add_argument('--target', type=str)      \n",
    "    parser.add_argument('--model', type=str)  \n",
    "    parser.add_argument('--featureset', type=str)    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "    featureset=args.featureset.split(',')\n",
    "    target_column=args.target\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', args.data_file)\n",
    "    train_data_output_path = os.path.join('/opt/ml/processing/output/training_data', 'training_%s.csv'%args.model)    \n",
    "    validation_data_output_path = os.path.join('/opt/ml/processing/output/validation_data', 'validation_%s.csv'%args.model)\n",
    "  \n",
    "    \n",
    "   \n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    dataset = pd.read_csv(input_data_path, error_bad_lines=False, index_col=False)\n",
    "    \n",
    "    \n",
    "    dataset=dataset[(dataset.cal_year < args.split_year)]\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "    for f in featureset:\n",
    "        X[f]=dataset.eval(f)\n",
    "    \n",
    "    y=dataset.eval(target_column)\n",
    "    \n",
    "\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=args.val_size, random_state=42)\n",
    "    \n",
    "    training_dataset=pd.DataFrame({'hasclaim':y_train}).join(X_train)\n",
    "    training_dataset.to_csv(train_data_output_path, header=False, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "    validation_dataset=pd.DataFrame({'hasclaim':y_val}).join(X_val)   \n",
    "    validation_dataset.to_csv(validation_data_output_path, header=False, index=False)\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Creating datasets for model FWaterClaims\n",
      "usagetype_encd,cova_deductible,cova_limit ,sqft,cal_year - yearbuilt,landlordind,water_risk_3_blk,constructioncd_encd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2020-12-31-20-05-41-583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-12-31-20-05-41-583\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://kdproperty/Data/dwelling_basedata_v4.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-757107622481/sagemaker-scikit-learn-2020-12-31-20-05-41-583/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'training_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://kdproperty/Data/training_data/FWaterClaims', 'LocalPath': '/opt/ml/processing/output/training_data', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://kdproperty/Data/validation_data/FWaterClaims', 'LocalPath': '/opt/ml/processing/output/validation_data', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processors=list()\n",
    "\n",
    "for index, row in models.iterrows():\n",
    "    model=row['Model']\n",
    "    print (index, ': Creating datasets for model %s'%model)\n",
    "    featureset=row[1:51].tolist()\n",
    "    featureset=[x for x in featureset if str(x) != 'nan']\n",
    "    print(','.join(featureset))\n",
    "    data_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type=instance_type,\n",
    "                                     instance_count=instance_count)\n",
    "\n",
    "    data_processor.run(code='preprocessing.py',\n",
    "                        inputs=[ProcessingInput(\n",
    "                        source='s3://%s%s'%(bucket,path_to_data_file+data_file),\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                        outputs=[ProcessingOutput(output_name='training_data', source='/opt/ml/processing/output/training_data',destination='s3://%s%straining_data/%s'%(bucket,path_to_data_file,model)),                                 \n",
    "                                 ProcessingOutput(output_name='validation_data', source='/opt/ml/processing/output/validation_data',destination='s3://%s%svalidation_data/%s'%(bucket,path_to_data_file,model)),                                                          \n",
    "                                ],\n",
    "                        arguments=['--data_file',data_file,\n",
    "                                 '--split_year',split_year,\n",
    "                                 '--val_size',val_size, \n",
    "                                 '--target',target,                                    \n",
    "                                 '--model',model,                                  \n",
    "                                 '--featureset', ','.join(featureset).replace(' ','')],\n",
    "                       experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name,\n",
    "        'TrialComponentDisplayName' : '%s-%s'%(Trial_name,model.replace('_','-')),},\n",
    "                    wait=False\n",
    "                     )\n",
    "    processors.append(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job sagemaker-scikit-learn-2020-12-31-20-05-41-583 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-05-41-583 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-05-41-583 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-05-41-583 status: InProgress\n",
      "Continue waiting...\n",
      "All Processing Jobs are Completed\n"
     ]
    }
   ],
   "source": [
    "#wait till the rest of processing jobs complete\n",
    "eu.wait_processing_jobs(processors=processors,check_every_sec=10,print_every_n_output=10,wait_min=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all jobs are done I want to have the list of training and validation data created in each job/trail component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Trial Component</th>\n",
       "      <th>Training_data</th>\n",
       "      <th>Validation_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FWaterClaims</td>\n",
       "      <td>FWaterClaims-PreparingTrainValidData-FWaterClaims</td>\n",
       "      <td>s3://kdproperty/Data/training_data/FWaterClaims</td>\n",
       "      <td>s3://kdproperty/Data/validation_data/FWaterClaims</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                                    Trial Component  \\\n",
       "0  FWaterClaims  FWaterClaims-PreparingTrainValidData-FWaterClaims   \n",
       "\n",
       "                                     Training_data  \\\n",
       "0  s3://kdproperty/Data/training_data/FWaterClaims   \n",
       "\n",
       "                                     Validation_data  \n",
       "0  s3://kdproperty/Data/validation_data/FWaterClaims  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=Experiment_name\n",
    ")\n",
    "trial_comp_ds = trial_component_analytics.dataframe()\n",
    "trial_ds=trial_comp_ds[trial_comp_ds['DisplayName'].str.contains(Trial_name)].copy()\n",
    "trial_ds['Model']=trial_ds['DisplayName'].str.replace(Trial_name+'-','')\n",
    "trial_ds['Model']=trial_ds['Model'].str.replace('-','_')\n",
    "trial_ds=trial_ds[['Model','DisplayName','training_data - Value','validation_data - Value']]\n",
    "trial_ds.columns=['Model','Trial Component','Training_data','Validation_data']\n",
    "trial_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no actual file name in Training_data and Validation_data. I can add it based on model's names but it's not needed. Training job will use whatever is in S3 bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into the Experiment log file names of created training and validation datasets in S3 to train models in other module \n",
    "eu.SaveToExperimentLog(Experiments_file, '%s InputData'%Experiment_name, trial_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
