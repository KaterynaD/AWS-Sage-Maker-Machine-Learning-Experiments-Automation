{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiments_file='/home/kate/Research/YearBuilt/Experiments/DwellingExperiments.xlsx'\n",
    "AllExperiments_tab='Experiments'\n",
    "Experiment_name='PartDepFWC'\n",
    "#Experiment configuration:  features we want to produce PartialDependency\n",
    "#each line in the file contains a feature name and a type:Categorical or Continuous\n",
    "#Categorical features values will be process all as is. Continuous only sorted(np.linspace(np.percentile(dataset_temp[f], 0.1),np.percentile(dataset_temp[f], 99.5),50))\n",
    "Experiment_Features_tab='%s Features'%Experiment_name\n",
    "#The full set of features the tested model is based on\n",
    "#each line in the file contains the model name and set of features to built a dataset for SageMaker\n",
    "#only one,first row is taken into account\n",
    "Experiment_tab='%s Models'%Experiment_name\n",
    "\n",
    "#Looks like Trial name should be unique in my environment, not in the experiment it belongs to\n",
    "Trial_name='%s-PreparingData'%Experiment_name\n",
    " \n",
    "bucket='kdproperty'\n",
    "path_to_data_file='/Data/'\n",
    "\n",
    "\n",
    "instance_type='ml.t3.2xlarge'\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/kate/Research/YearBuilt/Notebooks/Property')\n",
    "import ExperimentsUtils as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartDepFWC is a new experiment. nothing to delete\n"
     ]
    }
   ],
   "source": [
    "#delete experiment and trials but not output files associated with jobs in experiments\n",
    "eu.cleanup_experiment(Experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "eu.create_experiment(Experiment_name)\n",
    "eu.create_trial(Experiment_name,Trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasclaim_water\n",
      "dwelling_basedata_v4.csv\n"
     ]
    }
   ],
   "source": [
    "experiments = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=AllExperiments_tab)\n",
    "target=experiments[experiments['Experiment']==Experiment_name]['Target'].values[0]\n",
    "print(target)\n",
    "data_file=experiments[experiments['Experiment']==Experiment_name]['Dataset'].values[0]\n",
    "print(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cal_year - yearbuilt</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature        Type\n",
       "0  cal_year - yearbuilt  Continuous"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_Features_tab)\n",
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usagetype_encd',\n",
       " 'cova_deductible',\n",
       " 'cova_limit ',\n",
       " 'sqft',\n",
       " 'cal_year - yearbuilt',\n",
       " 'landlordind',\n",
       " 'water_risk_3_blk',\n",
       " 'constructioncd_encd']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take into account only one,first row \n",
    "models = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_tab)\n",
    "for index, row in models.iterrows():\n",
    "    model_complete_featureset=row[1:51].tolist()\n",
    "    model_complete_featureset=[x for x in model_complete_featureset if str(x) != 'nan']\n",
    "    break\n",
    "model_complete_featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "role = 'arn:aws:iam::757107622481:role/service-role/AmazonSageMaker-ExecutionRole-20200819T131882'\n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessingDataForPartialDependency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessingDataForPartialDependency.py\n",
    "\n",
    "#The code creates a separate dataset for each feature with all possible combination of feature values and the rest of the data\n",
    "#dataset for SageMaker are the same structure: no headers, the first column is a target and the rest are features\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_file', type=str)\n",
    "    parser.add_argument('--model_complete_featureset', type=str)      \n",
    "    parser.add_argument('--featureset', type=str)    \n",
    "    parser.add_argument('--featuretypes', type=str)     \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "    model_complete_featureset=args.model_complete_featureset.split(',')\n",
    "    featureset=args.featureset.split(',')\n",
    "    featuretypes=args.featuretypes.split(',')\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', args.data_file)\n",
    "\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    dataset = pd.read_csv(input_data_path, error_bad_lines=False, index_col=False)\n",
    "    \n",
    "     \n",
    "   \n",
    "    for feature,ftype in zip(featureset,featuretypes):\n",
    "        print(feature,ftype)\n",
    "        dataset_feature = pd.DataFrame()    \n",
    "        #dataset_temp = dataset[model_complete_featureset].copy()  \n",
    "        dataset_temp = pd.DataFrame()\n",
    "        for f in model_complete_featureset:\n",
    "            dataset_temp[f]=dataset.eval(f)\n",
    "        if ftype=='Continuous':\n",
    "            # continuous\n",
    "            grid = sorted(np.linspace(np.percentile(dataset_temp[feature], 0.1),\n",
    "                       np.percentile(dataset_temp[feature], 99.5),\n",
    "                          50))\n",
    "        else:\n",
    "            #categorical\n",
    "            grid = sorted(dataset_temp[feature].unique())        \n",
    " \n",
    "        for i, val in enumerate(grid):\n",
    "            dataset_temp[feature] = val\n",
    "            dataset_feature=dataset_feature.append(dataset_temp)\n",
    "        #save in parts if large dataset\n",
    "        if ftype=='Continuous':\n",
    "            parts = np.array_split(dataset_feature, 5)\n",
    "            \n",
    "            for i,p in enumerate(parts):\n",
    "                output_data_path = os.path.join('/opt/ml/processing/output', '%s_%s.csv'%(feature,i))\n",
    "                p.to_csv(output_data_path,header=False,index=False)\n",
    "        else:   \n",
    "            output_data_path = os.path.join('/opt/ml/processing/output', '%s.csv'%feature)\n",
    "            dataset_feature.to_csv(output_data_path,header=False,index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2020-12-31-20-28-01-844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-12-31-20-28-01-844\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://kdproperty/Data/dwelling_basedata_v4.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-757107622481/sagemaker-scikit-learn-2020-12-31-20-28-01-844/input/code/preprocessingDataForPartialDependency.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://kdproperty/Data/PartialDependencyInputData/PartDepFWC', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processors=list()\n",
    "\n",
    "featureset=Features['Feature'].tolist()\n",
    "featuretypes=Features['Type'].tolist()\n",
    "\n",
    "data_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type=instance_type,\n",
    "                                     instance_count=instance_count)\n",
    "\n",
    "data_processor.run(code='preprocessingDataForPartialDependency.py',\n",
    "                        inputs=[ProcessingInput(\n",
    "                        source='s3://%s%s'%(bucket,path_to_data_file+data_file),\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                        outputs=[ProcessingOutput(output_name='output', source='/opt/ml/processing/output', destination='s3://%s%sPartialDependencyInputData/%s'%(bucket,path_to_data_file,Experiment_name)),                                                          \n",
    "                                ],\n",
    "                        arguments=['--data_file',data_file,\n",
    "                                 '--model_complete_featureset',','.join(model_complete_featureset).replace(' ',''),\n",
    "                                 '--featureset', ','.join(featureset).replace(' ',''),\n",
    "                                 '--featuretypes', ','.join(featuretypes).replace(' ','')],\n",
    "                        experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name,\n",
    "        'TrialComponentDisplayName' : '%s'%(Trial_name,),},\n",
    "                        wait=False\n",
    "                        )\n",
    "processors.append(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "Processing job sagemaker-scikit-learn-2020-12-31-20-28-01-844 status: InProgress\n",
      "Continue waiting...\n",
      "All Processing Jobs are Completed\n"
     ]
    }
   ],
   "source": [
    "#wait till the rest of processing jobs complete\n",
    "eu.wait_processing_jobs(processors=processors,check_every_sec=10,print_every_n_output=10,wait_min=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial Component</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PartDepFWC-PreparingData</td>\n",
       "      <td>s3://kdproperty/Data/PartialDependencyInputDat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Trial Component                                               Data\n",
       "0  PartDepFWC-PreparingData  s3://kdproperty/Data/PartialDependencyInputDat..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=Experiment_name\n",
    ")\n",
    "trial_comp_ds = trial_component_analytics.dataframe()\n",
    "trial_ds=trial_comp_ds[trial_comp_ds['DisplayName'].str.contains(Trial_name)].copy()\n",
    "trial_ds=trial_ds[['DisplayName','output - Value']]\n",
    "trial_ds.columns=['Trial Component','Data']\n",
    "trial_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into the Experiment log location of the created  dataset\n",
    "eu.SaveToExperimentLog(Experiments_file, '%s InputData'%Experiment_name, trial_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
