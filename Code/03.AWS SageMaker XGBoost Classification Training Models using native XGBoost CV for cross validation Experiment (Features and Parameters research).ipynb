{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS SageMaker XGBoost Classification Training Models using native XGBoost CV for cross validation Experiment (Features and Parameters research)\n",
    "\n",
    "Purpose: Based on the configured list of features and/or parameters train several models to compare results. Native XGBoost CV function is used to perform cross validation.\n",
    "\n",
    "The idea is to do a **deep**  model comparison with the same method (XGBoost Classification), target variable and dataset but different sets of features and/or parameters. \n",
    "\n",
    "The models can be trained in parallel and even with a large dataste, the result is available relatively soon comparing to sequntial run on a local server.\n",
    "\n",
    "The output is not just a model and score but also feature importance, test dataset evaluation score and training/validation errors to analyze overfitting.\n",
    "\n",
    "OpenSource XGBoost is used in the script and it can NOT be replaced with SageMaker built-in XGBoost. \n",
    "\n",
    "The advantage of using this notebook over the previoust one (02.AWS... Training Models with cross validation) is less complex data preparation step (we do not need to create folds in advance) and less parallel running training jobs. If each training job is short one, we do not spend time to create an additional instance. There is also a limit on parallel running large instances in AWS which can be easily approach in complex experiment configurations.\n",
    "\n",
    "Also, it's only one way, which can be used to do Hyperparameters tuning with CV in AWS as I understand the process as for now.\n",
    "\n",
    "#### Custom training script output\n",
    "\n",
    "CV output looks different from training output even if we use  standard XGBoost metrics (ROC-AUC). There is \"test\" instead of \"validation\": \n",
    "[1060]#011train-auc:0.72230+0.00097#011test-auc:0.69244+0.01114\n",
    "\n",
    "As a result, the script works, models trains, but to get the CV result from the script we need to implement our own proces to return results via files in S3 or any other approach you may think about. AWS SageMaker monitoring systems, charts and experiments do not recognize the output.\n",
    "\n",
    "There are 2 approaches to make AWS SageMaker fully functional:\n",
    " - Adjust callback functions output in CV to make it identical to the train output. This is relatively simple option. We may even use a custom evaluation function, just name it like a standard function: accuracy, auc, etc \n",
    " - Register custom metrics with metric_defintions (https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics-sdk).\n",
    " \n",
    "Note: there is no a mandatory need to implement one of the 2 approached if you are NOT interested in AWS SageMaker monitoring functionality. If the creation of a callback function or a custom ECR repository (see below) for custom metrics are too complex you can skip them and just use output files with CV results in S3.\n",
    " \n",
    "As on Mar/2021 open source, script mode, AWS SageMaker XGBoost is still recognized like a standard, built-in algorithm and prevent us using custom metric definitions.\n",
    "See https://github.com/aws/sagemaker-xgboost-container/issues/121\n",
    "\n",
    "The workaround is to create your own container from the official AWS Sagemaker Open Source XGBoost GitHub repository, host it in your own ECR repository, and use this image from Python SDK. \n",
    "\n",
    "**Steps to create your own ECR repository:**\n",
    "\n",
    "1. Install and configure aws-cli (https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html and https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\n",
    "\n",
    "2. Install docker (https://docs.docker.com/engine/install/ubuntu/) or just add your user to the docker group (Some steps below do not work when docker used via sudo)\n",
    "- sudo usermod -aG docker kate\n",
    "Activate the changes to groups\n",
    "- newgrp docker\n",
    "Verify that you can run docker commands without sudo\n",
    "- docker run hello-world\n",
    "3. Build container\n",
    "- git clone https://github.com/aws/sagemaker-xgboost-container\n",
    "- docker build -t xgboost-container-base:1.2-1-cpu-py3 -f docker/1.0-1/base/Dockerfile.cpu .\n",
    "- python setup.py bdist_wheel --universal\n",
    "- docker build -t preprod-xgboost-container:1.2-1-cpu-py3 -f docker/1.2-1/final/Dockerfile.cpu .\n",
    "4. Create ECR repository and push the above image.\n",
    "- eval $(aws ecr get-login --region  us-west-2 --no-include-email | sed 's|https://||')\n",
    "- aws ecr create-repository --repository-name sagemaker-xgboost --region us-west-2\n",
    "- docker tag preprod-xgboost-container:1.2-1-cpu-py3 XYZ.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.2-1-cpu-py3\n",
    "- docker push XYZ.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.2-1-cpu-py3\n",
    "5. Use image_uri=XYZ.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.2-1-cpu-py3 in XGBoost()\n",
    "\n",
    "\n",
    "#### Notebook Main steps:\n",
    "\n",
    "1. Experiment configuration. Instead of hardcoding datafile name, target variable, featuresets and parameters sets directly in the code I use an Excel file. Each tab with a predefined name contains featuresets for each model or parameter sets, etc. At the end, the code records the results back into the same excel file. Excel is used as an UI to easily configure experiment and simplify re-use of this notebook\n",
    "\n",
    "AWS SageMaker Experiment is used also but I did not find it's very useful to track the featuresets, processing and visaulaize the results (available in SageMaker notebook).  I need to average data before comparing and take into account sem.\n",
    "\n",
    "2. Preparing training and validation datasets - data preprocessing - in S3 in a format suitable for AWS Sagemaker. SKLearnProcessor and a processing job are used to create all datasets for all models in one process but the same can be done directly in the script and only the result can be moved to S3. If the datasets can be re-used from a previous experiment, only S3 location to the files can be configured instead.\n",
    "Usually, testing different featuretests requires creation individual datasets per testing model and different parameters can be tested on the same dataset.\n",
    "\n",
    "Folds are NOT created at this moment. It's done in XGBoost CV.\n",
    "\n",
    "3. CV for each model is done in parallel. The number of simultaneously running training jobs is contolled by a parameter (MaxNumOfRunningModels).\n",
    "\n",
    "4. Extracting results, visualization, performing t-test and saving to an experiment log file. This is done for averaged results from all folds per model. \n",
    "\n",
    "\n",
    "#### Known issues:\n",
    "1. Warnings after upgrading SageMaker to version 2:\n",
    "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
    "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
    "Looks like warnings from XGBoost open-source estimator. No clear information about the parameters\n",
    "2. All models artifacts (debugging info) are saved into output_path provided as a parameter to the estimator (expected only model.tar.gz) except source/sourcedir.tar.gz which is saved into root of a bucket from output_path. Previously everything except model.tar.gz was saved into default bucket. This is entry_point (training) script and the path is defined in SM_MODULE_DIR environment variable but there is no clue how to change it. \n",
    "3. As on Mar/2021 open source, script mode, AWS SageMaker XGBoost is still recognized like a standard, built-in algorithm and prevent us using custom metric definitions.\n",
    "See https://github.com/aws/sagemaker-xgboost-container/issues/121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder='/home/kate/Research/YearBuilt/Notebooks/Experiments_v2/tmp/'\n",
    "#Experiment_name must NOT contain underscore (_)\n",
    "Experiment_name='subsample'\n",
    "#Experiments log file\n",
    "Experiments_file='/home/kate/Research/YearBuilt/Experiments/TestExperiments.xlsx'\n",
    "#AllExperiments_tab is a table with a list of all experiments included in the log\n",
    "#Mandatory columns: Experiment (Experiment_name), Dataset(data file name), Target(target column name from Dataset)\n",
    "#The rest of the columns are not use in the code below. I usually add in a free form: objective,status,result,notebook name used to conduct the experiment\n",
    "AllExperiments_tab='Experiments'\n",
    "#Experiment configuration:\n",
    "#1.Experiment_Features_tab: differenet datasets to try\n",
    "#each line in the tab contains a model name and set of features to built a dataset for SageMaker\n",
    "#a feature can be an exact column name from the Dataset column in AllExperiments_tab or a calculation based on exact column names and eval pandas function\n",
    "#if the experiment objective is to try different parameters sets, all models (if more then 1) can have the same feature sets.\n",
    "Experiment_Features_tab='%s Features'%Experiment_name\n",
    "#2. Alternatively a set of data files with preprocessed data in S3 can be provided in a form:\n",
    "#Model,Training_data,Validation_data[, Testing_data, Testing_labels]\n",
    "Experiment_InputData_tab='%s InputData'%Experiment_name\n",
    "#3. Experiment_Params_tab: each line in the tab contains a model name and set of XGBoost parametersto apply to a model\n",
    "#the set of models should be consistent in Experiment_Features_tab and Experiment_Params_tab\n",
    "#parameters can be the same for all models or specific in each model\n",
    "Experiment_Params_tab='%s Params'%Experiment_name\n",
    "\n",
    "#Trial names in AWS Sage Maker experiment\n",
    "Trial_name_preprocessing='%s-PreparingTrainValidData'%Experiment_name\n",
    "Trial_name_training='%s-TrainingModels'%Experiment_name\n",
    "\n",
    "#everything stored in\n",
    "bucket='kdproperty'\n",
    "\n",
    "path_to_data='Data'\n",
    "path_to_training_data='Data/Experiments/%s/training'%Experiment_name\n",
    "path_to_validation_data='Data/Experiments/%s/validation'%Experiment_name\n",
    "path_to_testing_data='Data/Experiments/%s/testing'%Experiment_name\n",
    "path_to_testing_labels='Data/Experiments/%s/labels'%Experiment_name\n",
    "path_to_configuration='Config'\n",
    "path_to_models='Models/Experiments/%s'%Experiment_name\n",
    "\n",
    "#preprocessing parameters\n",
    "split_year='2020'\n",
    "\n",
    "#entry_point defines a script to be run for model training\n",
    "#the scripts have different ouput and metric defnitions should be adjusted accordingly\n",
    "entry_point='ModelCV.py' #uses XGBoost CV to return the final, standard, evaluation metric - auc. Use custom image_uri and metric defnitions. \n",
    "# ModelCV_Gini_EvalMetric.py uses XGBoost CV with custom evaluation metric - gini. Use custom image_uri and metric defnitions. \n",
    "\n",
    "\n",
    "#just a placeholder. the parameter can be commented in XGBoostbor just leave it as empty string. It's used when standard metrics are used.\n",
    "stnadard_image_uri=''\n",
    "#it's needed as a workaround to be able to work with custom metrics and scripts output in AWS Sagemaker montitor systems, charts and experiment\n",
    "custom_image_uri='XYZ.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.2-1-cpu-py3'\n",
    "\n",
    "\n",
    "#number of folds for CV\n",
    "num_folds='10'\n",
    "\n",
    "\n",
    "\n",
    "#level of details returning from CV\n",
    "#any Y return models from a best iteration\n",
    "#FeatureImportance Y/N\n",
    "GetFIFlg='N'\n",
    "#Scores for Test data (should be provided in fit \"test\" input) Y/N\n",
    "GetTestScoreFlg='N'\n",
    "#Prediction of Test data (should be provided in fit \"test\" input) Y/N\n",
    "GetTestPredFlg='N'  \n",
    "\n",
    "\n",
    "#Confidence level for t-test\n",
    "confidence_level=0.05\n",
    "\n",
    "preprocessing_instance_type='ml.t3.large'\n",
    "preprocessing_instance_count=1\n",
    "\n",
    "#Training parameters\n",
    "training_instance_type='ml.c5.xlarge'\n",
    "training_instance_count=1\n",
    "\n",
    "#How many simultaneously running training jobs we want to see in the system\n",
    "MaxNumOfRunningModels = 30\n",
    "#when a job is completes/failed or stopped a new one can be added Jobs status is checked periodically\n",
    "check_training_job_every_sec=10\n",
    "\n",
    "#What to do with th2 experiment (rest of running jobs) if a training job failed or stopped\n",
    "StopOnFailedModel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "\n",
    "import s3fs\n",
    "import tarfile\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.session import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "#for analyzing results: charts and t-test\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be run as a first step\n",
    "#role arn is used when run from a local machine\n",
    "sagemaker_execution_role = 'arn:aws:iam::XYZ:role/service-role/AmazonSageMaker-ExecutionRole-20200819T131882'\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3 = s3fs.S3FileSystem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Experiment is configured in an experiment log file (Excel file, in my case,  in different tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading an experiment configuration (Experiment_name) from an experiment log file (Experiments_file). Target and Dataset columns in AllExperiments_tab contain data file name used and target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=AllExperiments_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target of models in subsample experiment is hasclaim_water\n",
      "Datafile used in subsample experiment is dwelling_basedata_v4.csv\n"
     ]
    }
   ],
   "source": [
    "target=experiments[experiments['Experiment']==Experiment_name]['Target'].values[0]\n",
    "print('Target of models in %s experiment is %s'%(Experiment_name,target))\n",
    "data_file=experiments[experiments['Experiment']==Experiment_name]['Dataset'].values[0]\n",
    "print('Datafile used in %s experiment is %s'%(Experiment_name,data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Models based on individual datasets to be created, trained and compared in the experiment (Experiment_Features_tab) is a table with first column Model name (should be unique) and next columns [1:51] features to train the model. Feature is the exact column name from the dataset or a calculation based on exact column names and eval pandas function\n",
    "\n",
    "This configuration will be used to preprocess data and also need to be moved to S3 in csv format for easy reading in a preprocessing script if we use AWS SKLearnProcessor/job/instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model                  F1               F2    F3                F4  F5  \\\n",
       "0  BaseModel  cal_year-yearbuilt  cova_deductible  sqft  water_risk_3_blk NaN   \n",
       "\n",
       "   F6  \n",
       "0 NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_features = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_Features_tab)\n",
    "model_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we may need to get mapping between f0 - fN features in the dataset and feature importance output based on Modelname\n",
    "def GetMap(model):\n",
    "    feature_map={}\n",
    "    df=model_features[model_features['Model']==model].loc[:, model_features.columns != 'Model']\n",
    "    for i,c in enumerate(df.columns):\n",
    "        feature_map['f%s'%i]=df[c].values[0]\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a.Preprocessed data may already exists in an S3. Experiment configuration can provide the list of files per model. In this case (len(preprocessed_data)==0) the code skips all steps to preprocess data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preprocessed data may already exists in an S3. Experiment configuration can provide the list of files per model. In this case (len(preprocessed_data)==0) the code skips all steps to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_data = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_InputData_tab)\n",
    "    #preprocessed_data = pd.concat([preprocessed_data,model_features.drop('Model',axis=1)], axis=1)\n",
    "    preprocessed_data  \n",
    "except:\n",
    "    preprocessed_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b.Saving into S3 models configurations (sets of features) to be used in data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(preprocessed_data)==0:\n",
    "    Model_Config_file='%s.csv'%Experiment_name\n",
    "    Models_Config_path = os.path.join(temp_folder, Model_Config_file) \n",
    "\n",
    "    model_features.to_csv(Models_Config_path, header=True, index=False)\n",
    "\n",
    "\n",
    "    input_code = sagemaker_session.upload_data(\n",
    "        Models_Config_path,\n",
    "        bucket=bucket,\n",
    "        key_prefix=path_to_configuration\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model params to be used in training is a table with first column Model name (should be unique and corresponds to models in Experiment_Features_tab) and next columns are XGBoost parameters\n",
    "In a general case, all models can have the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>num_round</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>auc</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model        objective eval_metric booster  scale_pos_weight  \\\n",
       "0  BaseModel  binary:logistic         auc  gbtree               0.3   \n",
       "\n",
       "   colsample_bylevel  colsample_bytree   eta  max_depth  num_round  subsample  \n",
       "0                0.6               0.8  0.02          3       5000        0.6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = pd.read_excel(open(Experiments_file, 'rb'), sheet_name=Experiment_Params_tab)\n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Verification if we have the same set of models in both configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_from_model_features=model_features['Model'].tolist()\n",
    "models_from_model_params=model_params['Model'].tolist()\n",
    "if len([x for x in models_from_model_features if x not in models_from_model_params])!=0:\n",
    "    raise Exception('Different set of models in featuresets and parametersets!')\n",
    "if len(preprocessed_data)>0:\n",
    "    models_from_preprocessed_data=preprocessed_data['Model'].tolist()\n",
    "    if len([x for x in models_from_preprocessed_data if x not in models_from_model_params])!=0:\n",
    "        raise Exception('Different set of models in input data and parametersets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Creating experiments and trials in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append('/home/kate/Research/YearBuilt/Notebooks/Experiments')\n",
    "import ExperimentsUtils as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "eu.cleanup_experiment(Experiment_name)\n",
    "eu.create_experiment(Experiment_name)\n",
    "eu.create_trial(Experiment_name,Trial_name_preprocessing)\n",
    "eu.create_trial(Experiment_name,Trial_name_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We may not need an AWS SKLearnProcessor/job/instances for relatively small and medium datasets (unless our data are already in S3 and it takes time and money to download)\n",
    "What's important is to save the prepared datasets in a predefined S3 location to be used in training.\n",
    "In a case of really huge datasets and instensive, time consuming preprocessing, a separate SKLearnProcessor/job for each model can be created with more then 1 powerful instance.\n",
    "\n",
    "Preprocessing script below reads the data from the input dataset, model configurations (desired featuresets), seprate 2020 (split_year) as a test dataset (not used in the experiment, because the data may be not complete developed in the year). I does NOT split the rest of the data to training and validation folds because this is what XGBoost CV will do. \n",
    "\n",
    "The datasets are saved in AWS SageMaker form (first column is a target, no header) in csv format. The location and filenames are based on model names: folder name is a model name and file names also contain a model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing_for_cv_all_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing_for_cv_all_models.py\n",
    "\n",
    "#Training and Validation dataset for SageMaker are the same structure: no headers, the first column is a target and the rest are features\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_file', type=str)\n",
    "    parser.add_argument('--split_year', type=int)        \n",
    "    parser.add_argument('--target', type=str)      \n",
    "    parser.add_argument('--config_file', type=str)     \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "   \n",
    "    target_column=args.target\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', args.data_file)\n",
    "    config_data_path = os.path.join('/opt/ml/processing/config', args.config_file)\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    dataset = pd.read_csv(input_data_path, error_bad_lines=False, index_col=False)\n",
    "    dataset_test=dataset[(dataset.cal_year == args.split_year)]\n",
    "    dataset=dataset[(dataset.cal_year < args.split_year)]    \n",
    "    \n",
    "\n",
    "    print('Reading config data from {}'.format(config_data_path))\n",
    "    models = pd.read_csv(config_data_path, error_bad_lines=False, index_col=False)    \n",
    "    \n",
    "\n",
    "    #iterating thru config file with models and featureset\n",
    "    for index, row in models.iterrows():\n",
    "        model=row['Model']\n",
    "        print (index, ': Creating datasets for model %s'%model)\n",
    "        featureset=row[1:51].tolist()\n",
    "        featureset=[x for x in featureset if str(x) != 'nan']\n",
    "        print(','.join(featureset))\n",
    "        \n",
    "        #creating dataset for a model according to configured dataset\n",
    "        X = pd.DataFrame()\n",
    "        X_test = pd.DataFrame()        \n",
    "        for f in featureset:\n",
    "            X[f]=dataset.eval(f)\n",
    "            X_test[f]=dataset_test.eval(f)            \n",
    "        y=dataset.eval(target_column)\n",
    "        y_test=dataset_test.eval(target_column) \n",
    "        \n",
    "        #Testing data starts from y_test because they are read in XGBoost processing script to DMatrix amd first column is separated anyway\n",
    "        #Without the column the script can not predict\n",
    "        print('Testing data...')\n",
    "        test_data_output_path = '/opt/ml/processing/output/testing_data/%s/'%model              \n",
    "        if not os.path.exists(test_data_output_path):\n",
    "            os.makedirs(test_data_output_path)       \n",
    "        test_data_output_path = os.path.join(test_data_output_path,  'testing_%s.csv'%(model))  \n",
    "        test_dataset=pd.DataFrame({target_column:y_test}).join(X_test)\n",
    "        test_dataset.to_csv(test_data_output_path, header=False, index=False)\n",
    "        \n",
    "        #The rest of teh data will be used in cv-fold as a whole and seprated to training/validation insode cv\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        print('Train data...')        \n",
    "        if not os.path.exists('/opt/ml/processing/output/training_data/%s'%model):\n",
    "            os.makedirs('/opt/ml/processing/output/training_data/%s'%model)\n",
    "        train_data_output_path = os.path.join('/opt/ml/processing/output/training_data/%s'%model, 'training_%s.csv'%model) \n",
    "        \n",
    "        training_dataset=pd.DataFrame({target_column:y_train}).join(X_train)\n",
    "        training_dataset.to_csv(train_data_output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing output (training and testing datasets) are saved separately for each model in a folder with the same name as a models name configured in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already preprocessed in S3\n"
     ]
    }
   ],
   "source": [
    "if len(preprocessed_data)==0:\n",
    "    data_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=sagemaker_execution_role,\n",
    "                                     instance_type=preprocessing_instance_type,\n",
    "                                     instance_count=preprocessing_instance_count)    \n",
    "    data_processor.run(code='preprocessing_for_cv_all_models.py',\n",
    "                            inputs=[ProcessingInput(input_name='data',source='s3://%s/%s/%s'%(bucket,path_to_data,data_file),destination='/opt/ml/processing/input'),\n",
    "            ProcessingInput(input_name='config',source='s3://%s/%s/%s'%(bucket,path_to_configuration,Model_Config_file),destination='/opt/ml/processing/config'),\n",
    "                                   ],\n",
    "                        outputs=[ProcessingOutput(output_name='training_data', source='/opt/ml/processing/output/training_data',destination='s3://%s/%s/'%(bucket,path_to_training_data)),                                 \n",
    "                                 ProcessingOutput(output_name='testing_data', source='/opt/ml/processing/output/testing_data',destination='s3://%s/%s/'%(bucket,path_to_testing_data)),                                                                                                                                                                                                                       \n",
    "                                ],\n",
    "                        arguments=['--data_file',data_file,\n",
    "                                 '--split_year',split_year,\n",
    "                                 '--target',target,                                    \n",
    "                                 '--config_file',Model_Config_file],\n",
    "                        experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name_preprocessing,\n",
    "        'TrialComponentDisplayName' : '%s-%s'%(Trial_name_preprocessing,'-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())),},\n",
    "                    wait=True\n",
    "                     )\n",
    "else:\n",
    "    print('Data already preprocessed in S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in a case a previous step started preproceesin (nothing provided in InputData) Stop the execution if there is an issue with creating input data for the models\n",
    "if len(preprocessed_data)==0:\n",
    "    job_name=data_processor.jobs[-1].describe()['ProcessingJobName']\n",
    "    if not(sagemaker_session.was_processing_job_successful(job_name)):\n",
    "        raise Exception('Preprocessing job Failed!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing output is saved separately for each model in a folder with the same name as a models name configured in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(preprocessed_data)==0:\n",
    "    preprocessed_data = pd.DataFrame(columns=['Model', 'Training_data',  'Testing_data'])\n",
    "    for index, row in model_features.iterrows():\n",
    "        model=row['Model']\n",
    "        train_input = 's3://%s/%s/%s/'%(bucket,path_to_training_data,model)\n",
    "        test_data = 's3://%s/%s/%s/testing_%s.csv'%(bucket,path_to_testing_data,model,model)\n",
    "        preprocessed_data.loc[index]=[model, train_input,test_data]\n",
    "    preprocessed_data = pd.concat([preprocessed_data,model_features.drop('Model',axis=1)], axis=1)    \n",
    "    #Saving into the Experiment log file names of created training and validation datasets in S3\n",
    "    eu.SaveToExperimentLog(Experiments_file, '%s InputData'%Experiment_name, preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training_data</th>\n",
       "      <th>Testing_data</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tra...</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tes...</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model                                      Training_data  \\\n",
       "0  BaseModel  s3://kdproperty/Data/Experiments/subsample/tra...   \n",
       "\n",
       "                                        Testing_data                  F1  \\\n",
       "0  s3://kdproperty/Data/Experiments/subsample/tes...  cal_year-yearbuilt   \n",
       "\n",
       "                F2    F3                F4  F5  F6  \n",
       "0  cova_deductible  sqft  water_risk_3_blk NaN NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Custom script to train a model. It's requred for open-source SageMaker XGBoost container used further in the notebook. The script returns some additional information (feature importance, test dataset scores and train/validation errors) from training for custom processing.  The script uses a call back function to access the models and return them from the function. The models are needed for feature importance, predict and score test data. The first model is saved as model.tar.gz as in a usual training job. All models are saved in output.tar.gz with the rest of the additional data.\n",
    "- ModelCV.py uses standard XGBoost evaluation metric(AUC) in CV\n",
    "- ModelCV_Gini_EvalMetric.py uses custome evaluation metric(gini) in CV\n",
    "You need just one script, configured in entry_point above, and used in XGBoost below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ModelCV.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ModelCV.py\n",
    "#ModelCV.py uses standard XGBoost evaluation metric(AUC) in CV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "from sagemaker_xgboost_container import distributed\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import re\n",
    "\n",
    "def cv_misc_callback(oof_train_scores:list, oof_valid_scores:list, best_models:list, NeedModelsFlg='N', maximize=True):\n",
    "    \"\"\"\n",
    "    It's called inside XGB CV to catch individual folds scores\n",
    "    \"\"\"    \n",
    "    state = {}\n",
    "    def init(env):\n",
    "        if maximize:\n",
    "            state['best_score'] = -np.inf\n",
    "        else:\n",
    "            state['best_score'] = np.inf \n",
    "    def callback(env):\n",
    "        #fold best model if flag\n",
    "        if NeedModelsFlg=='Y':\n",
    "            if not state:\n",
    "                init(env)\n",
    "            best_score = state['best_score']\n",
    "            score = env.evaluation_result_list[-1][1]\n",
    "            if (maximize and score > best_score) or (not maximize and score < best_score):\n",
    "                for i, cvpack in enumerate(env.cvfolds): \n",
    "                    best_models[i]=cvpack.bst\n",
    "                state['best_score'] = score    \n",
    "        #all iterations folds scores\n",
    "        folds_train_scores = []\n",
    "        folds_valid_scores = []\n",
    "        for i, cvpack in enumerate(env.cvfolds):\n",
    "            scores = cvpack.eval(iteration=0,feval=None)\n",
    "            scores_l = re.split(': |\\t',scores)\n",
    "            train_score=scores_l[1].rpartition(':')[2]\n",
    "            valid_score=scores_l[2].rpartition(':')[2]\n",
    "            folds_train_scores.append(train_score)\n",
    "            folds_valid_scores.append(valid_score)\n",
    "        oof_train_scores.append(folds_train_scores)\n",
    "        oof_valid_scores.append(folds_valid_scores)\n",
    "    callback.before_iteration = False\n",
    "    return callback\n",
    "\n",
    "def _xgb_cv(params, dtrain,  num_boost_round, nfold, early_stopping_rounds, model_dir, output_data_dir, GetFIFlg,GetTestScoreFlg,GetTestPredFlg,is_master):\n",
    "    \"\"\"Run xgb cv on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run will include this argument.\n",
    "    \"\"\"\n",
    "    oof_train_scores = []\n",
    "    oof_valid_scores = []\n",
    "    best_models=[None]*nfold\n",
    "    NeedModelsFlg = 'Y' if 'Y' in (GetFIFlg,GetTestScoreFlg,GetTestPredFlg) else 'N'\n",
    "    cv_results=xgb.cv(params, dtrain, num_boost_round=num_boost_round,\n",
    "                 nfold=nfold, stratified=True, shuffle=True,early_stopping_rounds=early_stopping_rounds, seed=42\n",
    "                      ,callbacks=[cv_misc_callback(oof_train_scores, oof_valid_scores,best_models,NeedModelsFlg,True), xgb.callback.print_evaluation(period=20)]\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "    #scores to dataframe\n",
    "    df_oof_train_scores = pd.DataFrame.from_records(oof_train_scores).apply(pd.to_numeric)\n",
    "    df_oof_valid_scores = pd.DataFrame.from_records(oof_valid_scores).apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "    \n",
    "    #only folds scores columns names\n",
    "    columns = df_oof_train_scores.columns.tolist()\n",
    "\n",
    "    \n",
    "    #mean and std, sem \n",
    "    df_oof_train_scores['std'] = df_oof_train_scores[columns].std(axis=1)\n",
    "    df_oof_valid_scores['std'] = df_oof_valid_scores[columns].std(axis=1)\n",
    "    df_oof_train_scores['sem'] = df_oof_train_scores[columns].sem(axis=1)\n",
    "    df_oof_valid_scores['sem'] = df_oof_valid_scores[columns].sem(axis=1)    \n",
    "    df_oof_train_scores['mean'] = df_oof_train_scores[columns].mean(axis=1)\n",
    "    df_oof_valid_scores['mean'] = df_oof_valid_scores[columns].mean(axis=1)\n",
    "    \n",
    "    #best models feature importance \n",
    "    if GetFIFlg=='Y':\n",
    "        oof_fi_weight_best = {}\n",
    "        oof_fi_gain_best = {}\n",
    "        oof_fi_cover_best = {}\n",
    "        for i in range(0,nfold):\n",
    "            oof_fi_weight_best[i]=best_models[i].get_score(importance_type='weight')\n",
    "            oof_fi_gain_best[i]= best_models[i].get_score(importance_type='gain')\n",
    "            oof_fi_cover_best[i]= best_models[i].get_score(importance_type='cover')\n",
    "    \n",
    "        #converting to dataframe\n",
    "        df_oof_fi_weight_best = pd.DataFrame(oof_fi_weight_best).apply(pd.to_numeric)\n",
    "        df_oof_fi_gain_best = pd.DataFrame(oof_fi_gain_best).apply(pd.to_numeric)\n",
    "        df_oof_fi_cover_best = pd.DataFrame(oof_fi_cover_best).apply(pd.to_numeric)\n",
    "    \n",
    "    \n",
    "        #mean and std, sem \n",
    "        df_oof_fi_weight_best['std'] = df_oof_fi_weight_best[columns].std(axis=1)\n",
    "        df_oof_fi_gain_best['std'] = df_oof_fi_gain_best[columns].std(axis=1)\n",
    "        df_oof_fi_cover_best['std'] = df_oof_fi_cover_best[columns].std(axis=1)\n",
    "    \n",
    "        df_oof_fi_weight_best['sem'] = df_oof_fi_weight_best[columns].sem(axis=1)\n",
    "        df_oof_fi_gain_best['sem'] = df_oof_fi_gain_best[columns].sem(axis=1)\n",
    "        df_oof_fi_cover_best['sem'] = df_oof_fi_cover_best[columns].sem(axis=1)\n",
    "    \n",
    "        df_oof_fi_weight_best['mean'] = df_oof_fi_weight_best[columns].mean(axis=1)\n",
    "        df_oof_fi_gain_best['mean'] = df_oof_fi_gain_best[columns].mean(axis=1)\n",
    "        df_oof_fi_cover_best['mean'] = df_oof_fi_cover_best[columns].mean(axis=1)\n",
    "    \n",
    "    \n",
    "        #feature codes from index to column\n",
    "        df_oof_fi_weight_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_weight_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "        df_oof_fi_gain_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_gain_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "        df_oof_fi_cover_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_cover_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "    \n",
    "    if 'Y' in (GetTestScoreFlg,GetTestPredFlg):\n",
    "        #Prediction on test data  from folds best models...\n",
    "        df_prediction=pd.DataFrame()\n",
    "        df_prediction['actual']=dtest.get_label()\n",
    "        for i in range(0,nfold):\n",
    "            df_prediction[i]=best_models[i].predict(dtest)\n",
    "   \n",
    "        #Test scores from test prediction   \n",
    "        df_scores = pd.DataFrame()\n",
    "        for i in range(0,nfold):\n",
    "            df_scores[i]=[roc_auc_score(df_prediction['actual'], df_prediction[i])]\n",
    "\n",
    "        df_scores['std'] = df_scores[columns].std(axis=1)\n",
    "        df_scores['sem'] = df_scores[columns].sem(axis=1)\n",
    "        df_scores['mean'] = df_scores[columns].mean(axis=1)\n",
    "\n",
    "    if is_master:\n",
    "        if not os.path.exists(output_data_dir):\n",
    "            os.makedirs(output_data_dir)\n",
    "            \n",
    "        if NeedModelsFlg == 'Y':\n",
    "            model_location = model_dir + '/xgboost-model'\n",
    "            pkl.dump(best_models[0], open(model_location, 'wb'))\n",
    "            print('Stored best model from 1st fold at {}'.format(model_location))\n",
    "            logging.info('Stored best model from 1st fold at {}'.format(model_location))        \n",
    "               \n",
    "            print('Stored best models from all folds at {}'.format(output_data_dir))\n",
    "            logging.info('Stored best models from all folds at {}'.format(output_data_dir))\n",
    "        \n",
    "            for i in range(0,nfold):\n",
    "                model_location = output_data_dir + '/xgboost-model-fold'+str(i)\n",
    "                pkl.dump(best_models[i], open(model_location, 'wb'))\n",
    "        \n",
    "        if  GetTestPredFlg=='Y':    \n",
    "            predictions_location = os.path.join(output_data_dir, 'test_predictions.csv')\n",
    "            print('Saving test predictions at {}'.format(predictions_location))\n",
    "            logging.info('Saving test predictions at {}'.format(predictions_location))            \n",
    "            df_prediction.to_csv(predictions_location, header=True, index=False)\n",
    "        \n",
    "        if  GetTestScoreFlg=='Y':\n",
    "            oof_test_scores_location = os.path.join(output_data_dir, 'oof_test_scores.csv')\n",
    "            print('Saving oof_test_scores at {}'.format(oof_test_scores_location))\n",
    "            logging.info('Saving oof_test_scores at {}'.format(oof_test_scores_location))\n",
    "            df_scores.to_csv(oof_test_scores_location, header=True, index=False)\n",
    "        \n",
    "        cv_result_location = os.path.join(output_data_dir, 'cv_results.csv')\n",
    "        print('Saving cv results at {}'.format(cv_result_location))\n",
    "        logging.info('Saving cv results at {}'.format(cv_result_location))\n",
    "        cv_results.to_csv(cv_result_location, header=True, index=False)\n",
    "        \n",
    "        oof_train_scores_location = os.path.join(output_data_dir, 'oof_train_scores.csv')\n",
    "        print('Saving oof_train_scores at {}'.format(oof_train_scores_location))\n",
    "        logging.info('Saving oof_train_scores at {}'.format(oof_train_scores_location))\n",
    "        df_oof_train_scores.to_csv(oof_train_scores_location, header=True, index=False)  \n",
    "        \n",
    "        oof_valid_scores_location = os.path.join(output_data_dir, 'oof_valid_scores.csv')\n",
    "        print('Saving oof_valid_scores at {}'.format(oof_valid_scores_location))\n",
    "        logging.info('Saving oof_valid_scores at {}'.format(oof_valid_scores_location))\n",
    "        df_oof_valid_scores.to_csv(oof_valid_scores_location, header=True, index=False)\n",
    "        \n",
    "        if  GetFIFlg=='Y':\n",
    "            oof_fi_weight_best_location = os.path.join(output_data_dir, 'oof_fi_weight_best.csv')\n",
    "            print('Saving oof_fi_weight_best at {}'.format(oof_fi_weight_best_location))\n",
    "            logging.info('Saving oof_fi_weight_best at {}'.format(oof_fi_weight_best_location))\n",
    "            df_oof_fi_weight_best.to_csv(oof_fi_weight_best_location, header=True, index=False)  \n",
    "        \n",
    "            oof_fi_gain_best_location = os.path.join(output_data_dir, 'oof_fi_gain_best.csv')\n",
    "            print('Saving oof_fi_gain_best at {}'.format(oof_fi_gain_best_location))\n",
    "            logging.info('Saving oof_fi_gain_best at {}'.format(oof_fi_gain_best_location))\n",
    "            df_oof_fi_gain_best.to_csv(oof_fi_gain_best_location, header=True, index=False)        \n",
    "        \n",
    "            oof_fi_cover_best_location = os.path.join(output_data_dir, 'oof_fi_cover_best.csv')\n",
    "            print('Saving oof_fi_cover_best at {}'.format(oof_fi_cover_best_location))\n",
    "            logging.info('Saving oof_fi_cover_best at {}'.format(oof_fi_cover_best_location))\n",
    "            df_oof_fi_cover_best.to_csv(oof_fi_cover_best_location, header=True, index=False)  \n",
    "           \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--nfold', type=int)\n",
    "    parser.add_argument('--early_stopping_rounds', type=int)\n",
    "    parser.add_argument('--booster', type=str)\n",
    "    parser.add_argument('--eval_metric', type=str)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--scale_pos_weight', type=float)\n",
    "    parser.add_argument('--colsample_bylevel', type=float)\n",
    "    parser.add_argument('--colsample_bytree', type=float)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--max_delta_step', type=int)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    \n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    \n",
    "    parser.add_argument('--GetFIFlg', type=str, default='N')\n",
    "    parser.add_argument('--GetTestScoreFlg', type=str, default='N')\n",
    "    parser.add_argument('--GetTestPredFlg', type=str, default='N')\n",
    "                \n",
    "                \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host\n",
    "\n",
    "    dtrain = get_dmatrix(args.train, 'csv')\n",
    "    \n",
    "    dtest = get_dmatrix(args.test, 'csv')\n",
    "\n",
    "    if not(dtest):\n",
    "        if ((args.GetTestScoreFlg=='Y') | (args.GetTestPredFlg=='Y')):\n",
    "            raise Exception('Please provide test data in a test channel for prediction and scores or set GetTestScoreFlg and GetTestPredFlg to N')\n",
    "\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'objective': args.objective,\n",
    "        'booster': args.booster,\n",
    "        'seed': args.seed,\n",
    "        'eval_metric':args.eval_metric,\n",
    "        'scale_pos_weight':args.scale_pos_weight,\n",
    "        'colsample_bylevel': args.colsample_bylevel,\n",
    "        'colsample_bytree': args.colsample_bytree,\n",
    "        'subsample': args.subsample,\n",
    "        'max_delta_step':args.max_delta_step\n",
    "        }\n",
    "\n",
    "    xgb_train_args = dict(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=args.num_round,\n",
    "        nfold=args.nfold, \n",
    "        early_stopping_rounds=args.early_stopping_rounds,\n",
    "        model_dir=args.model_dir,\n",
    "        output_data_dir=args.output_data_dir,\n",
    "        GetFIFlg=args.GetFIFlg,\n",
    "        GetTestScoreFlg=args.GetTestScoreFlg,\n",
    "        GetTestPredFlg=args.GetTestPredFlg\n",
    "    )\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(\n",
    "            exec_fun=_xgb_cv,\n",
    "            args=xgb_train_args,\n",
    "            include_in_training=(dtrain is not None),\n",
    "            hosts=sm_hosts,\n",
    "            current_host=sm_current_host,\n",
    "            update_rabit_args=True\n",
    "        )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_cv(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ModelCV_Gini_EvalMetric.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ModelCV_Gini_EvalMetric.py\n",
    "#ModelCV_Gini_EvalMetric.py uses ustome evaluation metric(gini) in CV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "from sagemaker_xgboost_container import distributed\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "def gini_xgb(pred, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, pred) / gini(y, y)\n",
    "\n",
    "import re\n",
    "\n",
    "def cv_misc_callback(oof_train_scores:list, oof_valid_scores:list, best_models:list, NeedModelsFlg='N', maximize=True):\n",
    "    \"\"\"\n",
    "    It's called inside XGB CV to catch individual folds scores\n",
    "    \"\"\"    \n",
    "    state = {}\n",
    "    def init(env):\n",
    "        if maximize:\n",
    "            state['best_score'] = -np.inf\n",
    "        else:\n",
    "            state['best_score'] = np.inf \n",
    "    def callback(env):\n",
    "        #fold best model if flag\n",
    "        if NeedModelsFlg=='Y':\n",
    "            if not state:\n",
    "                init(env)\n",
    "            best_score = state['best_score']\n",
    "            score = env.evaluation_result_list[-1][1]\n",
    "            if (maximize and score > best_score) or (not maximize and score < best_score):\n",
    "                for i, cvpack in enumerate(env.cvfolds): \n",
    "                    best_models[i]=cvpack.bst\n",
    "                state['best_score'] = score    \n",
    "        #all iterations folds scores\n",
    "        folds_train_scores = []\n",
    "        folds_valid_scores = []\n",
    "        for i, cvpack in enumerate(env.cvfolds):\n",
    "            scores = cvpack.eval(iteration=0,feval=gini_xgb)\n",
    "            scores_l = re.split(': |\\t',scores)\n",
    "            train_score=scores_l[1].rpartition(':')[2]\n",
    "            valid_score=scores_l[2].rpartition(':')[2]\n",
    "            folds_train_scores.append(train_score)\n",
    "            folds_valid_scores.append(valid_score)\n",
    "        oof_train_scores.append(folds_train_scores)\n",
    "        oof_valid_scores.append(folds_valid_scores)\n",
    "    callback.before_iteration = False\n",
    "    return callback\n",
    "\n",
    "def _xgb_cv(params, dtrain,  num_boost_round, nfold, early_stopping_rounds, model_dir, output_data_dir, GetFIFlg,GetTestScoreFlg,GetTestPredFlg,is_master):\n",
    "    \"\"\"Run xgb cv on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run will include this argument.\n",
    "    \"\"\"\n",
    "    oof_train_scores = []\n",
    "    oof_valid_scores = []\n",
    "    best_models=[None]*nfold\n",
    "    NeedModelsFlg = 'Y' if 'Y' in (GetFIFlg,GetTestScoreFlg,GetTestPredFlg) else 'N'\n",
    "    cv_results=xgb.cv(params, \n",
    "                      dtrain, \n",
    "                      feval=gini_xgb,\n",
    "                      num_boost_round=num_boost_round,\n",
    "                      nfold=nfold, \n",
    "                      stratified=True, \n",
    "                      shuffle=True,\n",
    "                      early_stopping_rounds=early_stopping_rounds, \n",
    "                      seed=42,\n",
    "                      callbacks=[cv_misc_callback(oof_train_scores, oof_valid_scores,best_models,NeedModelsFlg,True), xgb.callback.print_evaluation(period=1)]\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "    #scores to dataframe\n",
    "    df_oof_train_scores = pd.DataFrame.from_records(oof_train_scores).apply(pd.to_numeric)\n",
    "    df_oof_valid_scores = pd.DataFrame.from_records(oof_valid_scores).apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "    \n",
    "    #only folds scores columns names\n",
    "    columns = df_oof_train_scores.columns.tolist()\n",
    "\n",
    "    \n",
    "    #mean and std, sem \n",
    "    df_oof_train_scores['std'] = df_oof_train_scores[columns].std(axis=1)\n",
    "    df_oof_valid_scores['std'] = df_oof_valid_scores[columns].std(axis=1)\n",
    "    df_oof_train_scores['sem'] = df_oof_train_scores[columns].sem(axis=1)\n",
    "    df_oof_valid_scores['sem'] = df_oof_valid_scores[columns].sem(axis=1)    \n",
    "    df_oof_train_scores['mean'] = df_oof_train_scores[columns].mean(axis=1)\n",
    "    df_oof_valid_scores['mean'] = df_oof_valid_scores[columns].mean(axis=1)\n",
    "    \n",
    "    #best models feature importance \n",
    "    if GetFIFlg=='Y':\n",
    "        oof_fi_weight_best = {}\n",
    "        oof_fi_gain_best = {}\n",
    "        oof_fi_cover_best = {}\n",
    "        for i in range(0,nfold):\n",
    "            oof_fi_weight_best[i]=best_models[i].get_score(importance_type='weight')\n",
    "            oof_fi_gain_best[i]= best_models[i].get_score(importance_type='gain')\n",
    "            oof_fi_cover_best[i]= best_models[i].get_score(importance_type='cover')\n",
    "    \n",
    "        #converting to dataframe\n",
    "        df_oof_fi_weight_best = pd.DataFrame(oof_fi_weight_best).apply(pd.to_numeric)\n",
    "        df_oof_fi_gain_best = pd.DataFrame(oof_fi_gain_best).apply(pd.to_numeric)\n",
    "        df_oof_fi_cover_best = pd.DataFrame(oof_fi_cover_best).apply(pd.to_numeric)\n",
    "    \n",
    "    \n",
    "        #mean and std, sem \n",
    "        df_oof_fi_weight_best['std'] = df_oof_fi_weight_best[columns].std(axis=1)\n",
    "        df_oof_fi_gain_best['std'] = df_oof_fi_gain_best[columns].std(axis=1)\n",
    "        df_oof_fi_cover_best['std'] = df_oof_fi_cover_best[columns].std(axis=1)\n",
    "    \n",
    "        df_oof_fi_weight_best['sem'] = df_oof_fi_weight_best[columns].sem(axis=1)\n",
    "        df_oof_fi_gain_best['sem'] = df_oof_fi_gain_best[columns].sem(axis=1)\n",
    "        df_oof_fi_cover_best['sem'] = df_oof_fi_cover_best[columns].sem(axis=1)\n",
    "    \n",
    "        df_oof_fi_weight_best['mean'] = df_oof_fi_weight_best[columns].mean(axis=1)\n",
    "        df_oof_fi_gain_best['mean'] = df_oof_fi_gain_best[columns].mean(axis=1)\n",
    "        df_oof_fi_cover_best['mean'] = df_oof_fi_cover_best[columns].mean(axis=1)\n",
    "    \n",
    "    \n",
    "        #feature codes from index to column\n",
    "        df_oof_fi_weight_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_weight_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "        df_oof_fi_gain_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_gain_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "        df_oof_fi_cover_best.reset_index(level=0, inplace=True)\n",
    "        df_oof_fi_cover_best.columns=['feature'] + columns + ['std','sem','mean']\n",
    "    \n",
    "    if 'Y' in (GetTestScoreFlg,GetTestPredFlg):\n",
    "        #Prediction on test data  from folds best models...\n",
    "        df_prediction=pd.DataFrame()\n",
    "        df_prediction['actual']=dtest.get_label()\n",
    "        for i in range(0,nfold):\n",
    "            df_prediction[i]=best_models[i].predict(dtest)\n",
    "   \n",
    "        #Test scores from test prediction   \n",
    "        df_scores = pd.DataFrame()\n",
    "        for i in range(0,nfold):\n",
    "            df_scores[i]=[gini_xgb(df_prediction['actual'], df_prediction[i])]\n",
    "\n",
    "        df_scores['std'] = df_scores[columns].std(axis=1)\n",
    "        df_scores['sem'] = df_scores[columns].sem(axis=1)\n",
    "        df_scores['mean'] = df_scores[columns].mean(axis=1)\n",
    "\n",
    "    if is_master:\n",
    "        if not os.path.exists(output_data_dir):\n",
    "            os.makedirs(output_data_dir)\n",
    "            \n",
    "        if NeedModelsFlg == 'Y':\n",
    "            model_location = model_dir + '/xgboost-model'\n",
    "            pkl.dump(best_models[0], open(model_location, 'wb'))\n",
    "            print('Stored best model from 1st fold at {}'.format(model_location))\n",
    "            logging.info('Stored best model from 1st fold at {}'.format(model_location))        \n",
    "               \n",
    "            print('Stored best models from all folds at {}'.format(output_data_dir))\n",
    "            logging.info('Stored best models from all folds at {}'.format(output_data_dir))\n",
    "        \n",
    "            for i in range(0,nfold):\n",
    "                model_location = output_data_dir + '/xgboost-model-fold'+str(i)\n",
    "                pkl.dump(best_models[i], open(model_location, 'wb'))\n",
    "        \n",
    "        if  GetTestPredFlg=='Y':    \n",
    "            predictions_location = os.path.join(output_data_dir, 'test_predictions.csv')\n",
    "            print('Saving test predictions at {}'.format(predictions_location))\n",
    "            logging.info('Saving test predictions at {}'.format(predictions_location))            \n",
    "            df_prediction.to_csv(predictions_location, header=True, index=False)\n",
    "        \n",
    "        if  GetTestScoreFlg=='Y':\n",
    "            oof_test_scores_location = os.path.join(output_data_dir, 'oof_test_scores.csv')\n",
    "            print('Saving oof_test_scores at {}'.format(oof_test_scores_location))\n",
    "            logging.info('Saving oof_test_scores at {}'.format(oof_test_scores_location))\n",
    "            df_scores.to_csv(oof_test_scores_location, header=True, index=False)\n",
    "        \n",
    "        cv_result_location = os.path.join(output_data_dir, 'cv_results.csv')\n",
    "        print('Saving cv results at {}'.format(cv_result_location))\n",
    "        logging.info('Saving cv results at {}'.format(cv_result_location))\n",
    "        cv_results.to_csv(cv_result_location, header=True, index=False)\n",
    "        \n",
    "        oof_train_scores_location = os.path.join(output_data_dir, 'oof_train_scores.csv')\n",
    "        print('Saving oof_train_scores at {}'.format(oof_train_scores_location))\n",
    "        logging.info('Saving oof_train_scores at {}'.format(oof_train_scores_location))\n",
    "        df_oof_train_scores.to_csv(oof_train_scores_location, header=True, index=False)  \n",
    "        \n",
    "        oof_valid_scores_location = os.path.join(output_data_dir, 'oof_valid_scores.csv')\n",
    "        print('Saving oof_valid_scores at {}'.format(oof_valid_scores_location))\n",
    "        logging.info('Saving oof_valid_scores at {}'.format(oof_valid_scores_location))\n",
    "        df_oof_valid_scores.to_csv(oof_valid_scores_location, header=True, index=False)\n",
    "        \n",
    "        if  GetFIFlg=='Y':\n",
    "            oof_fi_weight_best_location = os.path.join(output_data_dir, 'oof_fi_weight_best.csv')\n",
    "            print('Saving oof_fi_weight_best at {}'.format(oof_fi_weight_best_location))\n",
    "            logging.info('Saving oof_fi_weight_best at {}'.format(oof_fi_weight_best_location))\n",
    "            df_oof_fi_weight_best.to_csv(oof_fi_weight_best_location, header=True, index=False)  \n",
    "        \n",
    "            oof_fi_gain_best_location = os.path.join(output_data_dir, 'oof_fi_gain_best.csv')\n",
    "            print('Saving oof_fi_gain_best at {}'.format(oof_fi_gain_best_location))\n",
    "            logging.info('Saving oof_fi_gain_best at {}'.format(oof_fi_gain_best_location))\n",
    "            df_oof_fi_gain_best.to_csv(oof_fi_gain_best_location, header=True, index=False)        \n",
    "        \n",
    "            oof_fi_cover_best_location = os.path.join(output_data_dir, 'oof_fi_cover_best.csv')\n",
    "            print('Saving oof_fi_cover_best at {}'.format(oof_fi_cover_best_location))\n",
    "            logging.info('Saving oof_fi_cover_best at {}'.format(oof_fi_cover_best_location))\n",
    "            df_oof_fi_cover_best.to_csv(oof_fi_cover_best_location, header=True, index=False)  \n",
    "           \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--nfold', type=int)\n",
    "    parser.add_argument('--early_stopping_rounds', type=int)\n",
    "    parser.add_argument('--booster', type=str)\n",
    "    parser.add_argument('--eval_metric', type=str)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--scale_pos_weight', type=float)\n",
    "    parser.add_argument('--colsample_bylevel', type=float)\n",
    "    parser.add_argument('--colsample_bytree', type=float)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--max_delta_step', type=int)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    \n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    \n",
    "    parser.add_argument('--GetFIFlg', type=str, default='N')\n",
    "    parser.add_argument('--GetTestScoreFlg', type=str, default='N')\n",
    "    parser.add_argument('--GetTestPredFlg', type=str, default='N')\n",
    "                \n",
    "                \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host\n",
    "\n",
    "    dtrain = get_dmatrix(args.train, 'csv')\n",
    "    \n",
    "    dtest = get_dmatrix(args.test, 'csv')\n",
    "\n",
    "    if not(dtest):\n",
    "        if ((args.GetTestScoreFlg=='Y') | (args.GetTestPredFlg=='Y')):\n",
    "            raise Exception('Please provide test data in a test channel for prediction and scores or set GetTestScoreFlg and GetTestPredFlg to N')\n",
    "\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'objective': args.objective,\n",
    "        'booster': args.booster,\n",
    "        'seed': args.seed,\n",
    "        #'eval_metric':args.eval_metric,\n",
    "        'disable_default_eval_metric': '1',\n",
    "        'scale_pos_weight':args.scale_pos_weight,\n",
    "        'colsample_bylevel': args.colsample_bylevel,\n",
    "        'colsample_bytree': args.colsample_bytree,\n",
    "        'subsample': args.subsample,\n",
    "        'max_delta_step':args.max_delta_step\n",
    "        }\n",
    "\n",
    "    xgb_train_args = dict(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=args.num_round,\n",
    "        nfold=args.nfold, \n",
    "        early_stopping_rounds=args.early_stopping_rounds,\n",
    "        model_dir=args.model_dir,\n",
    "        output_data_dir=args.output_data_dir,\n",
    "        GetFIFlg=args.GetFIFlg,\n",
    "        GetTestScoreFlg=args.GetTestScoreFlg,\n",
    "        GetTestPredFlg=args.GetTestPredFlg\n",
    "    )\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(\n",
    "            exec_fun=_xgb_cv,\n",
    "            args=xgb_train_args,\n",
    "            include_in_training=(dtrain is not None),\n",
    "            hosts=sm_hosts,\n",
    "            current_host=sm_current_host,\n",
    "            update_rabit_args=True\n",
    "        )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_cv(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For each parameter set we create an estimator and train it using training and validation datasets created in previous step and saved in a predefined location \n",
    "based on Model name. \n",
    "\n",
    "The train and valid files locations are saved in preprocessed_data dataframe. They are created for each fold.\n",
    "\n",
    "Since we built our training jobs based on preconfigured parameters and train/valid locations the data in 2 configuration must be consistent (the same model names).\n",
    "\n",
    "Only configured number (MaxNumOfRunningModels) of models is running at the same time. The process starts initial MaxNumOfRunningModels models and waits till \n",
    "one of them Complete, Failed or Stopped (StopOnFailedModel=False only).\n",
    "If a model training job Failed or Stopped and StopOnFailedModel is True, the whole process is broken.\n",
    "Since the training and validation data are created for each fold, the resulting table (data_for_training) will consist data for each configured feature set, parameter set and all folds.\n",
    "The total number of training jobs is number of featuresets * folds * number of parametersets. It can be huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>num_round</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Training_data</th>\n",
       "      <th>Testing_data</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>auc</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tra...</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tes...</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model        objective eval_metric booster  scale_pos_weight  \\\n",
       "0  BaseModel  binary:logistic         auc  gbtree               0.3   \n",
       "\n",
       "   colsample_bylevel  colsample_bytree   eta  max_depth  num_round  subsample  \\\n",
       "0                0.6               0.8  0.02          3       5000        0.6   \n",
       "\n",
       "                                       Training_data  \\\n",
       "0  s3://kdproperty/Data/Experiments/subsample/tra...   \n",
       "\n",
       "                                        Testing_data                  F1  \\\n",
       "0  s3://kdproperty/Data/Experiments/subsample/tes...  cal_year-yearbuilt   \n",
       "\n",
       "                F2    F3                F4  F5  F6  \n",
       "0  cova_deductible  sqft  water_risk_3_blk NaN NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_from_preprocessed_data=preprocessed_data['Model'].tolist()\n",
    "models_from_model_params=model_params['Model'].tolist()\n",
    "if len([x for x in models_from_preprocessed_data if x not in models_from_model_params])!=0:\n",
    "    raise Exception('Different set of models in preprocessed_data and parametersets!')\n",
    "#using merge because, in general, we can have different number of rows in each dataframe - folds in data and different sets of params\n",
    "data_for_training=pd.merge(model_params, preprocessed_data, on='Model', how='inner')\n",
    "data_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use AWS SageMaker montoring system, charts and AWS SageMaker experiment then custom metrics (CV output is always custom) should be registered. The metrics work only with a custom XGBoost image. See more detail at the top how to create it.\n",
    "Comment using metric_definitions and image_uri in the next cell (look for xgb_script_mode_estimator = XGBoost) if you are Ok with the custom script output and do not use/need AWS SageMaker monitoring features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if entry_point=='ModelCV.py':\n",
    "    #[1060]#011train-auc:0.72230+0.00097#011test-auc:0.69244+0.01114\n",
    "    metric_definitions = [\n",
    "    {\n",
    "        'Name': 'train-auc',\n",
    "        'Regex': '.*\\\\[[0-9]+\\\\].*#011train-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'\n",
    "    },    \n",
    "    {\n",
    "        'Name': 'test-auc',\n",
    "        'Regex': '.*\\\\[[0-9]+\\\\].*#011test-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'\n",
    "    }\n",
    "] \n",
    "    image_uri=custom_image_uri\n",
    "elif entry_point=='ModelCV_Gini_EvalMetric.py':    \n",
    "    #[99]#011train-gini:0.10383+0.08569#011test-gini:0.10243+0.08417\n",
    "    metric_definitions = [\n",
    "    {\n",
    "        'Name': 'train-gini',\n",
    "        'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gini:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'\n",
    "    },    \n",
    "    {\n",
    "        'Name': 'test-gini',\n",
    "        'Regex': '.*\\\\[[0-9]+\\\\].*#011test-gini:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'\n",
    "    }\n",
    "]\n",
    "    image_uri=custom_image_uri\n",
    "else:\n",
    "    metric_definitions = []\n",
    "    image_uri=standard_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training job for  BaseModel-0 model\n",
      "{'early_stopping_rounds': 100, 'seed': 42, 'nfold': '10', 'GetFIFlg': 'N', 'GetTestScoreFlg': 'N', 'GetTestPredFlg': 'N', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'booster': 'gbtree', 'scale_pos_weight': 0.3, 'colsample_bylevel': 0.6, 'colsample_bytree': 0.8, 'eta': 0.02, 'max_depth': 3, 'num_round': 5000, 'subsample': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: BaseModel-0-2021-03-22-15-23-18\n"
     ]
    }
   ],
   "source": [
    "CntRunningInst = 0\n",
    "\n",
    "processors=list()\n",
    "training_jobs_names=list()\n",
    "\n",
    "#regexpression to exclude features (F1..F25) from the list of parameters\n",
    "regex = re.compile('F[ 0-9]')\n",
    "\n",
    "for index, row in data_for_training.iterrows():\n",
    "    model='%s-%s'%(row['Model'],index)\n",
    "    print(model)\n",
    "    #1. Verifying training and validation data exists (were created in preprocessing step or moved manually in a predefined location)\n",
    "\n",
    "    if not(s3.exists(row['Training_data'])):           \n",
    "        print('Training data does not exist. Skipping model %s'%(model))\n",
    "        print('Check Training data in %s'%row['Training_data'])             \n",
    "        continue\n",
    "    #2. Do we have available instances slots to run the model? It depends on number of allowed in teh account simultaneously running specific instance types,\n",
    "    #number of instance type configured per model and configured number of model running\n",
    "    if CntRunningInst >= MaxNumOfRunningModels * training_instance_count: #not enough slots to add a model\n",
    "        print('There is no slot to train  %s model. Waiting...'%model)\n",
    "        #Waiting till a taining job complete\n",
    "        while CntRunningInst >= MaxNumOfRunningModels * training_instance_count:\n",
    "            print('.', end='')\n",
    "            time.sleep(check_training_job_every_sec)            \n",
    "            for p in processors:\n",
    "                ModelFailed=False\n",
    "                name=p.jobs[-1].describe()['TrainingJobName']\n",
    "                status=p.jobs[-1].describe()['TrainingJobStatus']\n",
    "                dummyFlag=not(StopOnFailedModel)\n",
    "                #job completed, failed or stopped (and do not stop the process) then we a slot is free\n",
    "                if (\n",
    "                    (status=='Completed') | \n",
    "                    ( ((status=='Failed') | \n",
    "                       (status=='Stopped')\n",
    "                      ) & \n",
    "                      dummyFlag\n",
    "                    )\n",
    "                   ):\n",
    "                    print('')\n",
    "                    print('Job %s  is %s'%(name,status))\n",
    "                    print('Continue training...')\n",
    "                    CntRunningInst = CntRunningInst - training_instance_count\n",
    "                    processors.remove(p)\n",
    "                    break\n",
    "                elif ( ((status=='Failed') | (status=='Stopped')) & StopOnFailedModel) :\n",
    "                    raise Exception('Model %s training failed!'%name)\n",
    "    #3. there is a slot to add a model training job\n",
    "    print('Creating training job for  %s model'%model)\n",
    "    #parameters\n",
    "    #techically nfold and early_stopping_rounds are not XGBoost parameters but it's an easy way to send them into the training/CV process\n",
    "    hyperparameters = {\n",
    "        'early_stopping_rounds':100,\n",
    "        'seed': 42,\n",
    "        'nfold': num_folds,\n",
    "        'GetFIFlg':GetFIFlg,\n",
    "        'GetTestScoreFlg':GetTestScoreFlg,\n",
    "        'GetTestPredFlg':GetTestPredFlg \n",
    "    } \n",
    "    for i, param in enumerate(data_for_training.columns):\n",
    "        #skip first column with Model name and dataset names or features\n",
    "        #if do not exclude then they will be added into experiment analytics as parameters but not used in training anyway\n",
    "        if ((param in ('Model','Training_data','Validation_data','Testing_data','Testing_labels')) | (bool(re.match(regex, param)))):\n",
    "            continue\n",
    "        hyperparameters[param] = row[param]\n",
    "    print(hyperparameters)\n",
    "    \n",
    "    #training and validation data from preprocessing\n",
    "    train_input = TrainingInput(row['Training_data'], content_type='text/csv')\n",
    "    #not required for CV Only if test prediction or scores are needed\n",
    "    test_input = TrainingInput(row['Testing_data'], content_type='text/csv')\n",
    "    \n",
    "    #Estimator\n",
    "\n",
    "    \n",
    "    training_job_name = model.replace('_','-')+'-'+time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "    training_jobs_names.append(training_job_name)\n",
    "    xgb_script_mode_estimator = XGBoost(\n",
    "        entry_point=entry_point,\n",
    "        image_uri=image_uri, #workaround to make sure custom metrics can be used - my own container, empty string ('') in image_uri force to use a standard image\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=sagemaker_execution_role, \n",
    "        instance_count=training_instance_count,\n",
    "        instance_type=training_instance_type,\n",
    "        framework_version='1.2-1',\n",
    "        output_path='s3://%s/%s/'%(bucket,path_to_models),\n",
    "        metric_definitions=metric_definitions #only workds if image_uri is a custom container\n",
    "        )\n",
    "    \n",
    "    #Training\n",
    "    xgb_script_mode_estimator.fit({'train': train_input, 'test': test_input}, job_name=training_job_name, wait=False,\n",
    "    experiment_config = {\n",
    "        'ExperimentName': Experiment_name ,\n",
    "        'TrialName' : Trial_name_training,\n",
    "        'TrialComponentDisplayName' : '%s-%s'%(Trial_name_training,model.replace('_','-')),})\n",
    "              \n",
    "    processors.append(xgb_script_mode_estimator)\n",
    "\n",
    "    CntRunningInst = CntRunningInst + training_instance_count\n",
    "    # to prevent throttling\n",
    "    time.sleep(.5)\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "Training job BaseModel-0-2021-03-22-15-23-18 status: InProgress\n",
      "Continue waiting...\n",
      "All Training Jobs are Completed\n"
     ]
    }
   ],
   "source": [
    "#Waiting till the rest of the training jobs are complete\n",
    "eu.wait_training_jobs(processors=processors,check_every_sec=10,print_every_n_output=20,wait_min=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment results\n",
    "\n",
    "Reading from AWS SageMaker experiment, saving to an experiment log file and visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the custom metrics allows SageMaker understand XGBoost CV output. The first visible result is populating test-auc and train-auc charts in Training job Monitor section. \n",
    "AWS SageMaker experiment is also can now read the output from XGBoost CV.\n",
    "To get more useful information from CV you may need to adjust \"period\" in the callback function. Use period=1 to get results from each iteration and make the results registered in AWS SageMaker experiment and produced from the custom scripts and output identical.\n",
    "\n",
    "xgb.callback.print_evaluation(period=20)\n",
    "\n",
    "The custom output uses an other callback function to get more detail data, available at each iteration from each fold separately. If print_evaluation skips the best iteration with max test score, AWS SageMaker Experiment will not be able to find it.\n",
    "\n",
    "test-auc - Count and train-auc - Count are Not the best iteration (even with period=1). It's best iteration plus early_stopping_rounds.\n",
    "test-auc - Max is the best validation metric. (Or Min for other types of evaluation scores like mae or rmse for regressions) test-auc - Last is the latest validation metric in the output. If early stopping round is used, they are usually almost the same with non-significant difference.\n",
    "However train-auc - Max (or - Min) is not the correspondent to validation train metrics. It's just selected indpendently from the output as the greatest (or least) train output. It maybe the same as train-auc - Last in a case if the latest 100 (early_stopping_rounds) rounds train metric is still changing but validation is still the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait till the data are updated in AWS experiment\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "#models training data from experiment \n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=Experiment_name   \n",
    ")\n",
    "trial_comp_ds = trial_component_analytics.dataframe()\n",
    "trial_ds=trial_comp_ds[trial_comp_ds['DisplayName'].str.contains(Trial_name_training)].copy()\n",
    "trial_ds['Model']=trial_ds['DisplayName'].str.replace(Trial_name_training+'-','')\n",
    "trial_ds['Model']=trial_ds['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TrialComponentName', 'DisplayName', 'SourceArn', 'GetFIFlg',\n",
       "       'GetTestPredFlg', 'GetTestScoreFlg', 'SageMaker.ImageUri',\n",
       "       'SageMaker.InstanceCount', 'SageMaker.InstanceType',\n",
       "       'SageMaker.VolumeSizeInGB', 'booster', 'colsample_bylevel',\n",
       "       'colsample_bytree', 'early_stopping_rounds', 'eta', 'eval_metric',\n",
       "       'max_depth', 'nfold', 'num_round', 'objective',\n",
       "       'sagemaker_container_log_level', 'sagemaker_job_name',\n",
       "       'sagemaker_program', 'sagemaker_region', 'sagemaker_submit_directory',\n",
       "       'scale_pos_weight', 'seed', 'subsample', 'test-auc - Min',\n",
       "       'test-auc - Max', 'test-auc - Avg', 'test-auc - StdDev',\n",
       "       'test-auc - Last', 'test-auc - Count', 'train-auc - Min',\n",
       "       'train-auc - Max', 'train-auc - Avg', 'train-auc - StdDev',\n",
       "       'train-auc - Last', 'train-auc - Count', 'test - MediaType',\n",
       "       'test - Value', 'train - MediaType', 'train - Value',\n",
       "       'SageMaker.DebugHookOutput - MediaType',\n",
       "       'SageMaker.DebugHookOutput - Value',\n",
       "       'SageMaker.ModelArtifact - MediaType',\n",
       "       'SageMaker.ModelArtifact - Value', 'Trials', 'Experiments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_comp_ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>train:auc</th>\n",
       "      <th>validation:auc</th>\n",
       "      <th>Trial Component</th>\n",
       "      <th>ModelFile</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>...</th>\n",
       "      <th>num_round</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Training_data</th>\n",
       "      <th>Testing_data</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0.70884</td>\n",
       "      <td>0.68327</td>\n",
       "      <td>subsample-TrainingModels-BaseModel-0</td>\n",
       "      <td>s3://kdproperty/Models/Experiments/subsample/B...</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>auc</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tra...</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/subsample/tes...</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  train:auc  validation:auc                       Trial Component  \\\n",
       "0  BaseModel    0.70884         0.68327  subsample-TrainingModels-BaseModel-0   \n",
       "\n",
       "                                           ModelFile        objective  \\\n",
       "0  s3://kdproperty/Models/Experiments/subsample/B...  binary:logistic   \n",
       "\n",
       "  eval_metric booster  scale_pos_weight  colsample_bylevel  ...  num_round  \\\n",
       "0         auc  gbtree               0.3                0.6  ...       5000   \n",
       "\n",
       "   subsample                                      Training_data  \\\n",
       "0        0.6  s3://kdproperty/Data/Experiments/subsample/tra...   \n",
       "\n",
       "                                        Testing_data                  F1  \\\n",
       "0  s3://kdproperty/Data/Experiments/subsample/tes...  cal_year-yearbuilt   \n",
       "\n",
       "                F2    F3                F4  F5  F6  \n",
       "0  cova_deductible  sqft  water_risk_3_blk NaN NaN  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelsResults = trial_ds[['Model','train-auc - Max','test-auc - Max','DisplayName','SageMaker.ModelArtifact - Value']].copy()\n",
    "ModelsResults.columns=['Model','train:auc','validation:auc','Trial Component','ModelFile']\n",
    "#index of data_for_training is saved in Model name as the last partition\n",
    "ModelsResults['ind']=pd.to_numeric(ModelsResults['Model'].apply(lambda x: x.rpartition('-')[2]))\n",
    "ModelsResults['Model']=ModelsResults['Model'].apply(lambda x: x.rpartition('-')[0])\n",
    "ModelsResults.sort_values('ind', ascending=True,inplace=True)\n",
    "ModelsResults = ModelsResults.set_index('ind')\n",
    "ModelsResults = pd.concat([ModelsResults, data_for_training.drop('Model',axis=1)], axis=1)\n",
    "ModelsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS SageMaker experiment output is not used in the script below. Custom output from teh training script has more useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training script, which is, in fact, does not train but rather run CV, produces 3 files in TrainingJob output location TrainingJobName/output/output.tar.gz:\n",
    "\n",
    "1.cv_results.csv is the standad output of XGBoost CV: training score-mean, training score-std, testing-score mean, testing score std for each iteration\n",
    "The data are already averaged by folds\n",
    "2.oof_train_scores.csv is the output from inside of XGBoost via a callback function. There are detail data for each fold with mean and std 0,1..num_folds,mean,std where 0,1..num_folds are each iteration output for each fold\n",
    "3.oof_test_scores.csv is the same as oof_train_scores.csv, but for validation portion of a fold\n",
    "\n",
    "Because oof_train_scores.csv and oof_test_scores.csv are extension of cv_results.csv and there is a wrong std in the standard cv_results I ignore cv_results.csv in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel-0-2021-03-22-15-23-18\n",
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "ModelTrainScores=pd.DataFrame()\n",
    "ModelTestScores=pd.DataFrame()\n",
    "ModelValidScores=pd.DataFrame()\n",
    "BestModelFI_gain=pd.DataFrame() \n",
    "BestModelFI_weight=pd.DataFrame() \n",
    "BestModelFI_cover=pd.DataFrame()\n",
    "ModelFiles = pd.DataFrame(columns=['Model', 'ind','Fold 0 Model','Output Data'])\n",
    "for p in training_jobs_names:\n",
    "    TrainingjobName=p\n",
    "    ModelOutput='s3://%s/%s/%s/output/output.tar.gz'%(bucket,path_to_models,TrainingjobName)\n",
    "    ModelFile='s3://%s/%s/%s/output/model.tar.gz'%(bucket,path_to_models,TrainingjobName)\n",
    "    l=TrainingjobName.split('-')\n",
    "    model=l[0]\n",
    "    ind=l[1]\n",
    "    print(TrainingjobName)\n",
    "    ModelFiles.loc[int(ind)]=[model, ind, ModelFile,ModelOutput]\n",
    "    if s3.exists(ModelOutput):\n",
    "        sagemaker_session.download_data(path=temp_folder, bucket=bucket, key_prefix=ModelOutput.replace('s3://%s/'%bucket,''))\n",
    "        print('Processing...')\n",
    "        results_file=os.path.join(temp_folder, 'output.tar.gz')\n",
    "        with tarfile.open(results_file) as tar:\n",
    "            tar.extractall(path=temp_folder)\n",
    "        #cv_results_file=os.path.join(temp_folder, 'cv_results.csv')\n",
    "        #cv_results=pd.read_csv(cv_results_file, error_bad_lines=False, index_col=False)\n",
    "        #cv_results['Model']=model\n",
    "        #cv_results['ind']=int(ind)\n",
    "        #CVResults = pd.concat([CVResults,cv_results])\n",
    "        #Training results folds\n",
    "        oof_train_scores_file=os.path.join(temp_folder, 'oof_train_scores.csv')\n",
    "        oof_train_scores=pd.read_csv(oof_train_scores_file, error_bad_lines=False, index_col=False)\n",
    "        oof_train_scores['Model']=model\n",
    "        oof_train_scores['ind']=int(ind)      \n",
    "        ModelTrainScores = pd.concat([ModelTrainScores,oof_train_scores])\n",
    "        #validing results folds\n",
    "        oof_valid_scores_file=os.path.join(temp_folder, 'oof_valid_scores.csv')\n",
    "        oof_valid_scores=pd.read_csv(oof_valid_scores_file, error_bad_lines=False, index_col=False)\n",
    "        oof_valid_scores['Model']=model\n",
    "        oof_valid_scores['ind']=int(ind)     \n",
    "        ModelValidScores = pd.concat([ModelValidScores,oof_valid_scores])\n",
    "        #Test Scores \n",
    "        if (GetTestScoreFlg=='Y'):\n",
    "            oof_test_scores_file=os.path.join(temp_folder, 'oof_test_scores.csv')\n",
    "            oof_test_scores=pd.read_csv(oof_test_scores_file, error_bad_lines=False, index_col=False)\n",
    "            oof_test_scores['Model']=model\n",
    "            oof_test_scores['ind']=int(ind) \n",
    "            ModelTestScores = pd.concat([ModelTestScores,oof_test_scores])\n",
    "        #FI\n",
    "        if (GetFIFlg=='Y'):\n",
    "            oof_fi_gain_best_file=os.path.join(temp_folder, 'oof_fi_gain_best.csv')\n",
    "            oof_fi_weight_best_file=os.path.join(temp_folder, 'oof_fi_weight_best.csv')    \n",
    "            oof_fi_cover_best_file=os.path.join(temp_folder, 'oof_fi_cover_best.csv')        \n",
    "            if ((GetFIFlg=='Y') & (os.path.isfile(oof_fi_gain_best_file)) & (os.path.isfile(oof_fi_weight_best_file)) & (os.path.isfile(oof_fi_cover_best_file))):\n",
    "                #FI gain\n",
    "                oof_fi_gain_best=pd.read_csv(oof_fi_gain_best_file, error_bad_lines=False, index_col=False)\n",
    "                oof_fi_gain_best['feature']=oof_fi_gain_best['feature'].map(GetMap(model))\n",
    "                oof_fi_gain_best['Model']=model\n",
    "                oof_fi_gain_best['ind']=int(ind)      \n",
    "                BestModelFI_gain = pd.concat([BestModelFI_gain,oof_fi_gain_best])\n",
    "                #FI weight        \n",
    "                oof_fi_weight_best=pd.read_csv(oof_fi_weight_best_file, error_bad_lines=False, index_col=False)\n",
    "                oof_fi_weight_best['feature']=oof_fi_weight_best['feature'].map(GetMap(model))\n",
    "                oof_fi_weight_best['Model']=model\n",
    "                oof_fi_weight_best['ind']=int(ind)      \n",
    "                BestModelFI_weight = pd.concat([BestModelFI_weight,oof_fi_weight_best])\n",
    "                #FI cover        \n",
    "                oof_fi_cover_best=pd.read_csv(oof_fi_cover_best_file, error_bad_lines=False, index_col=False)\n",
    "                oof_fi_cover_best['feature']=oof_fi_cover_best['feature'].map(GetMap(model))\n",
    "                oof_fi_cover_best['Model']=model\n",
    "                oof_fi_cover_best['ind']=int(ind)      \n",
    "                BestModelFI_cover = pd.concat([BestModelFI_cover,oof_fi_cover_best]) \n",
    "            else:\n",
    "                print('Feature Importance files not found')\n",
    "    else:\n",
    "        print('File does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of columns with folds scores depends on the number of folds (num_folds) We do not know in advance how many of them exist in the results\n",
    "folds_columns=[]\n",
    "folds_train_columns=[]\n",
    "folds_valid_columns=[]\n",
    "folds_gain_columns=[]\n",
    "folds_weight_columns=[]\n",
    "folds_cover_columns=[]\n",
    "for i in range(0,int(num_folds),1):\n",
    "    folds_columns.append(str(i))\n",
    "    folds_train_columns.append('train-%s-fold'%i)\n",
    "    folds_valid_columns.append('valid-%s-fold'%i)\n",
    "    folds_gain_columns.append('gain-%s'%i)\n",
    "    folds_weight_columns.append('weight-%s'%i)\n",
    "    folds_cover_columns.append('cover-%s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Training and validation errors from folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ind</th>\n",
       "      <th>train-0-fold</th>\n",
       "      <th>train-1-fold</th>\n",
       "      <th>train-2-fold</th>\n",
       "      <th>train-3-fold</th>\n",
       "      <th>train-4-fold</th>\n",
       "      <th>train-5-fold</th>\n",
       "      <th>train-6-fold</th>\n",
       "      <th>train-7-fold</th>\n",
       "      <th>...</th>\n",
       "      <th>valid-3-fold</th>\n",
       "      <th>valid-4-fold</th>\n",
       "      <th>valid-5-fold</th>\n",
       "      <th>valid-6-fold</th>\n",
       "      <th>valid-7-fold</th>\n",
       "      <th>valid-8-fold</th>\n",
       "      <th>valid-9-fold</th>\n",
       "      <th>valid-auc-mean</th>\n",
       "      <th>valid-auc-std</th>\n",
       "      <th>valid-auc-sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.706689</td>\n",
       "      <td>0.710077</td>\n",
       "      <td>0.708443</td>\n",
       "      <td>0.710565</td>\n",
       "      <td>0.708859</td>\n",
       "      <td>0.709122</td>\n",
       "      <td>0.709826</td>\n",
       "      <td>0.708980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673737</td>\n",
       "      <td>0.678408</td>\n",
       "      <td>0.683923</td>\n",
       "      <td>0.673623</td>\n",
       "      <td>0.683779</td>\n",
       "      <td>0.692952</td>\n",
       "      <td>0.688915</td>\n",
       "      <td>0.683032</td>\n",
       "      <td>0.011553</td>\n",
       "      <td>0.003654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.706688</td>\n",
       "      <td>0.710096</td>\n",
       "      <td>0.708463</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.708874</td>\n",
       "      <td>0.709138</td>\n",
       "      <td>0.709831</td>\n",
       "      <td>0.708995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673730</td>\n",
       "      <td>0.678380</td>\n",
       "      <td>0.683927</td>\n",
       "      <td>0.673657</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>0.692949</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.683043</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.003652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.706726</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.708474</td>\n",
       "      <td>0.710601</td>\n",
       "      <td>0.708879</td>\n",
       "      <td>0.709136</td>\n",
       "      <td>0.709861</td>\n",
       "      <td>0.709007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673715</td>\n",
       "      <td>0.678393</td>\n",
       "      <td>0.683907</td>\n",
       "      <td>0.673685</td>\n",
       "      <td>0.683808</td>\n",
       "      <td>0.692981</td>\n",
       "      <td>0.688957</td>\n",
       "      <td>0.683054</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.003651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.706733</td>\n",
       "      <td>0.710121</td>\n",
       "      <td>0.708501</td>\n",
       "      <td>0.710615</td>\n",
       "      <td>0.708927</td>\n",
       "      <td>0.709136</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.709015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673731</td>\n",
       "      <td>0.678399</td>\n",
       "      <td>0.683899</td>\n",
       "      <td>0.673608</td>\n",
       "      <td>0.683843</td>\n",
       "      <td>0.692989</td>\n",
       "      <td>0.688916</td>\n",
       "      <td>0.683053</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>0.003650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.706749</td>\n",
       "      <td>0.710138</td>\n",
       "      <td>0.708534</td>\n",
       "      <td>0.710648</td>\n",
       "      <td>0.708942</td>\n",
       "      <td>0.709170</td>\n",
       "      <td>0.709884</td>\n",
       "      <td>0.709035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.678428</td>\n",
       "      <td>0.683849</td>\n",
       "      <td>0.673637</td>\n",
       "      <td>0.683887</td>\n",
       "      <td>0.693007</td>\n",
       "      <td>0.688939</td>\n",
       "      <td>0.683050</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.003647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  ind  train-0-fold  train-1-fold  train-2-fold  train-3-fold  \\\n",
       "1158  BaseModel    0      0.706689      0.710077      0.708443      0.710565   \n",
       "1159  BaseModel    0      0.706688      0.710096      0.708463      0.710564   \n",
       "1160  BaseModel    0      0.706726      0.710098      0.708474      0.710601   \n",
       "1161  BaseModel    0      0.706733      0.710121      0.708501      0.710615   \n",
       "1162  BaseModel    0      0.706749      0.710138      0.708534      0.710648   \n",
       "\n",
       "      train-4-fold  train-5-fold  train-6-fold  train-7-fold  ...  \\\n",
       "1158      0.708859      0.709122      0.709826      0.708980  ...   \n",
       "1159      0.708874      0.709138      0.709831      0.708995  ...   \n",
       "1160      0.708879      0.709136      0.709861      0.709007  ...   \n",
       "1161      0.708927      0.709136      0.709877      0.709015  ...   \n",
       "1162      0.708942      0.709170      0.709884      0.709035  ...   \n",
       "\n",
       "      valid-3-fold  valid-4-fold  valid-5-fold  valid-6-fold  valid-7-fold  \\\n",
       "1158      0.673737      0.678408      0.683923      0.673623      0.683779   \n",
       "1159      0.673730      0.678380      0.683927      0.673657      0.683758   \n",
       "1160      0.673715      0.678393      0.683907      0.673685      0.683808   \n",
       "1161      0.673731      0.678399      0.683899      0.673608      0.683843   \n",
       "1162      0.673734      0.678428      0.683849      0.673637      0.683887   \n",
       "\n",
       "      valid-8-fold  valid-9-fold  valid-auc-mean  valid-auc-std  valid-auc-sem  \n",
       "1158      0.692952      0.688915        0.683032       0.011553       0.003654  \n",
       "1159      0.692949      0.688940        0.683043       0.011550       0.003652  \n",
       "1160      0.692981      0.688957        0.683054       0.011544       0.003651  \n",
       "1161      0.692989      0.688916        0.683053       0.011543       0.003650  \n",
       "1162      0.693007      0.688939        0.683050       0.011532       0.003647  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instead of a standard output from XGBoost CV I create an extended version\n",
    "CVResults=pd.concat([\n",
    "                     ModelTrainScores['Model'],\n",
    "                     ModelTrainScores['ind'],\n",
    "                     ModelTrainScores[folds_columns],\n",
    "                     ModelTrainScores['mean'],\n",
    "                     ModelTrainScores['std'], \n",
    "                     ModelTrainScores['sem'],  \n",
    "                     ModelValidScores[folds_columns],\n",
    "                     ModelValidScores['mean'],\n",
    "                     ModelValidScores['std'] ,\n",
    "                     ModelValidScores['sem'] ],\n",
    "                     axis=1)\n",
    "CVResults.columns=['Model','ind']+folds_train_columns+['train-auc-mean', 'train-auc-std', 'train-auc-sem']+folds_valid_columns+['valid-auc-mean', 'valid-auc-std', 'valid-auc-sem']\n",
    "CVResults.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Importance if it was generated in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestModelFI = pd.DataFrame()\n",
    "if (len(BestModelFI_gain)+len(BestModelFI_weight)+len(BestModelFI_cover)>0):\n",
    "    BestModelFI_gain.columns=['feature']+folds_gain_columns+['gain-std', 'gain-sem', 'gain-mean', 'Model', 'ind']\n",
    "    BestModelFI_gain=BestModelFI_gain[['Model', 'ind','feature']+folds_gain_columns+['gain-mean','gain-std', 'gain-sem']]\n",
    "    BestModelFI_weight.columns=['feature']+folds_weight_columns+['weight-std', 'weight-sem', 'weight-mean', 'Model', 'ind']\n",
    "    BestModelFI_weight=BestModelFI_weight[['Model', 'ind','feature']+folds_weight_columns+['weight-mean','weight-std', 'weight-sem']]\n",
    "    BestModelFI_cover.columns=['feature']+folds_cover_columns+['cover-std', 'cover-sem', 'cover-mean', 'Model', 'ind']\n",
    "    BestModelFI_cover=BestModelFI_cover[['Model', 'ind','feature']+folds_cover_columns+['cover-mean','cover-std', 'cover-sem']]\n",
    "    BestModelFI=pd.merge(BestModelFI_gain,\n",
    "                     BestModelFI_weight,\n",
    "                     on=['Model','ind','feature'], how='inner')\n",
    "    BestModelFI=pd.merge(BestModelFI,\n",
    "                     BestModelFI_cover,\n",
    "                     on=['Model','ind','feature'], how='inner')\n",
    "BestModelFI.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(BestModelFI): \n",
    "    for index, row in data_for_training.iterrows():\n",
    "        if len(BestModelFI[( (BestModelFI['Model']==row['Model']) & (BestModelFI['ind']==index))])>0:\n",
    "            data=BestModelFI[( (BestModelFI['Model']==row['Model']) & (BestModelFI['ind']==index))].sort_values('gain-mean',ascending=False)\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=3,figsize=(20,5)) \n",
    "            fig.suptitle('%s %s'%(row['Model'],index))\n",
    "                \n",
    "            ax = axs[0]\n",
    "            ax.errorbar(data['feature'], data['gain-mean'], color = 'blue',  ecolor='lightgray', elinewidth=3, capsize=0,yerr=data['gain-sem'], fmt='o')\n",
    "            ax.set_title('Gain')\n",
    "            ax.set_xticklabels(data['feature'].values,rotation=90)\n",
    "           \n",
    " \n",
    "            data=data.sort_values('weight-mean',ascending=False)\n",
    "            ax = axs[1]\n",
    "            ax.errorbar(data['feature'], data['weight-mean'], color = 'blue',  ecolor='lightgray', elinewidth=3, capsize=0,yerr=data['weight-sem'], fmt='o')\n",
    "            ax.set_title('Weight')\n",
    "            ax.set_xticklabels(data['feature'].values,rotation=90)\n",
    "                         \n",
    "            data=data.sort_values('cover-mean',ascending=False)\n",
    "            ax = axs[2]\n",
    "            ax.errorbar(data['feature'], data['cover-mean'], color = 'blue',  ecolor='lightgray', elinewidth=3, capsize=0,yerr=data['weight-sem'], fmt='o')\n",
    "            ax.set_title('Cover')\n",
    "            ax.set_xticklabels(data['feature'].values,rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(BestModelFI): \n",
    "    #Saving into the Experiment log file models results\n",
    "    eu.SaveToExperimentLog(Experiments_file, '%s FI'%Experiment_name, BestModelFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Visualization aggregated from folds best models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ind</th>\n",
       "      <th>TotalIterations</th>\n",
       "      <th>BestIteration</th>\n",
       "      <th>train-0-fold</th>\n",
       "      <th>train-1-fold</th>\n",
       "      <th>train-2-fold</th>\n",
       "      <th>train-3-fold</th>\n",
       "      <th>train-4-fold</th>\n",
       "      <th>train-5-fold</th>\n",
       "      <th>...</th>\n",
       "      <th>valid-3-fold</th>\n",
       "      <th>valid-4-fold</th>\n",
       "      <th>valid-5-fold</th>\n",
       "      <th>valid-6-fold</th>\n",
       "      <th>valid-7-fold</th>\n",
       "      <th>valid-8-fold</th>\n",
       "      <th>valid-9-fold</th>\n",
       "      <th>valid-auc-mean</th>\n",
       "      <th>valid-auc-std</th>\n",
       "      <th>valid-auc-sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>1163</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>0.708554</td>\n",
       "      <td>0.706692</td>\n",
       "      <td>0.708854</td>\n",
       "      <td>0.70731</td>\n",
       "      <td>0.707417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673897</td>\n",
       "      <td>0.678664</td>\n",
       "      <td>0.684572</td>\n",
       "      <td>0.673745</td>\n",
       "      <td>0.684495</td>\n",
       "      <td>0.692695</td>\n",
       "      <td>0.689111</td>\n",
       "      <td>0.683287</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.003586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  ind  TotalIterations  BestIteration  train-0-fold  train-1-fold  \\\n",
       "0  BaseModel    0             1163           1062      0.704921      0.708554   \n",
       "\n",
       "   train-2-fold  train-3-fold  train-4-fold  train-5-fold  ...  valid-3-fold  \\\n",
       "0      0.706692      0.708854       0.70731      0.707417  ...      0.673897   \n",
       "\n",
       "   valid-4-fold  valid-5-fold  valid-6-fold  valid-7-fold  valid-8-fold  \\\n",
       "0      0.678664      0.684572      0.673745      0.684495      0.692695   \n",
       "\n",
       "   valid-9-fold  valid-auc-mean  valid-auc-std  valid-auc-sem  \n",
       "0      0.689111        0.683287       0.011341       0.003586  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestResults=pd.DataFrame()\n",
    "for index, row in data_for_training.iterrows():\n",
    "    if len(CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index))])>0:\n",
    "        #max or mean depending on the score\n",
    "        BestTestScore=CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index))]['valid-auc-mean'].max()\n",
    "        #even if there are more then 1 rows with the same max valid-auc-mean use only first: head(1)\n",
    "        BestModelResult=CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index) & \n",
    "                                                       (CVResults['valid-auc-mean']==BestTestScore))].head(1).copy()\n",
    "        BestModelResult['TotalIterations']=CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index))].shape[0]\n",
    "        BestResults=pd.concat([BestResults,BestModelResult])\n",
    "BestResults.reset_index(inplace=True)\n",
    "BestResults.columns=['BestIteration','Model', 'ind']+folds_train_columns+['train-auc-mean', 'train-auc-std', 'train-auc-sem']+folds_valid_columns+['valid-auc-mean', 'valid-auc-std', 'valid-auc-sem','TotalIterations']\n",
    "BestResults=BestResults[['Model', 'ind','TotalIterations','BestIteration']+folds_train_columns+['train-auc-mean', 'train-auc-std', 'train-auc-sem']+folds_valid_columns+['valid-auc-mean', 'valid-auc-std', 'valid-auc-sem']]\n",
    "BestResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>valid-auc-mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707169</td>\n",
       "      <td>0.683287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-auc-mean  valid-auc-mean\n",
       "0        0.707169        0.683287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestResults[['train-auc-mean','valid-auc-mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVResults[( ((CVResults['ind']==index))]['valid-auc-mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.702186  0.700675  0.704014  0.703417  0.699948  0.702225  0.702131   \n",
      "0  0.696130  0.696052  0.697852  0.698487  0.693136  0.696680  0.695765   \n",
      "\n",
      "          7         8         9       std       sem      mean        Model  \\\n",
      "0  0.702510  0.704787  0.702863  0.001439  0.000455  0.702476    BaseModel   \n",
      "0  0.696543  0.696863  0.694265  0.001567  0.000495  0.696177  PropertyAge   \n",
      "\n",
      "   ind  \n",
      "0    0  \n",
      "0    1  \n"
     ]
    }
   ],
   "source": [
    "if len(ModelTestScores)>0:\n",
    "    print(ModelTestScores)\n",
    "    #Saving into the Experiment log file models results\n",
    "    eu.SaveToExperimentLog(Experiments_file, '%s TestScores'%Experiment_name, ModelTestScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Excluding from chart models which did not learn anything (0.5 is random guessing)\n",
    "data = BestResults[BestResults['valid-auc-mean']>0.5].copy()\n",
    "if len(ModelTestScores)>0:\n",
    "    data_test = ModelTestScores[ModelTestScores['mean']>0.5].copy()\n",
    "#list of models for xticks\n",
    "data['xticks']=data['Model']+' '+data['ind'].astype(str) \n",
    "xticks=data['xticks'].unique().tolist()\n",
    "\n",
    "\n",
    "# The x position \n",
    "r1 = np.arange(len(data))\n",
    "if len(ModelTestScores)>0:\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True,figsize=(20,10))\n",
    "else:\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True,figsize=(20,10))\n",
    "ax = axs[0]\n",
    "ax.errorbar(r1, data['valid-auc-mean'], color = 'cyan',  ecolor='lightgray', elinewidth=3, capsize=0,yerr=data['valid-auc-sem'], fmt='o')\n",
    "ax.set_title('valid-auc-mean')\n",
    "ax = axs[1]\n",
    "ax.errorbar(r1, data['train-auc-mean'],  color = 'blue',  ecolor='lightgray', elinewidth=3,capsize=0, yerr=data['train-auc-sem'],  fmt='o')\n",
    "ax.set_title('train-auc-mean')\n",
    "ax.set_xticks([r  for r in range(len(data))])\n",
    "ax.set_xticklabels(xticks,rotation=90)\n",
    "fig.suptitle('Means of AUC with standard error of the mean')\n",
    "if len(data_test)>0:\n",
    "    ax = axs[2]\n",
    "    ax.errorbar(r1, data_test['mean'],  color = 'green',  ecolor='lightgray', elinewidth=3,capsize=0, yerr=data_test['sem'],  fmt='o')\n",
    "    ax.set_title('test-auc-mean')\n",
    "    ax.set_xticks([r  for r in range(len(data_test))])\n",
    "    ax.set_xticklabels(xticks,rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. t-test\n",
    "Compares the scores of individual folds in a choosen model to the rest of the models folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a specific BaseModel name and index or just select with min or max score\n",
    "#The rest of the models will be compared to baseModel and baseind\n",
    "BaseModel=BestResults[BestResults['valid-auc-mean']==BestResults['valid-auc-mean'].max()]['Model'].values[0]\n",
    "BaseInd=BestResults[BestResults['valid-auc-mean']==BestResults['valid-auc-mean'].max()]['ind'].values[0]\n",
    "BaseModelResults=BestResults[((BestResults['Model']==BaseModel) & (BestResults['ind']==BaseInd))][folds_valid_columns].values[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init new columns for t-test results\n",
    "BestResults['t-statistic']=0.000\n",
    "BestResults['pvalue']=0.000\n",
    "BestResults['Comment']='Base Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ind</th>\n",
       "      <th>TotalIterations</th>\n",
       "      <th>BestIteration</th>\n",
       "      <th>train-0-fold</th>\n",
       "      <th>train-1-fold</th>\n",
       "      <th>train-2-fold</th>\n",
       "      <th>train-3-fold</th>\n",
       "      <th>train-4-fold</th>\n",
       "      <th>train-5-fold</th>\n",
       "      <th>...</th>\n",
       "      <th>valid-6-fold</th>\n",
       "      <th>valid-7-fold</th>\n",
       "      <th>valid-8-fold</th>\n",
       "      <th>valid-9-fold</th>\n",
       "      <th>valid-auc-mean</th>\n",
       "      <th>valid-auc-std</th>\n",
       "      <th>valid-auc-sem</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>1170</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.706434</td>\n",
       "      <td>0.709781</td>\n",
       "      <td>0.707839</td>\n",
       "      <td>0.709202</td>\n",
       "      <td>0.70888</td>\n",
       "      <td>0.709031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674756</td>\n",
       "      <td>0.685188</td>\n",
       "      <td>0.693142</td>\n",
       "      <td>0.685688</td>\n",
       "      <td>0.684884</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.003321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Base Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PropertyAge</td>\n",
       "      <td>1</td>\n",
       "      <td>1163</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>0.708554</td>\n",
       "      <td>0.706692</td>\n",
       "      <td>0.708854</td>\n",
       "      <td>0.70731</td>\n",
       "      <td>0.707417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673745</td>\n",
       "      <td>0.684495</td>\n",
       "      <td>0.692695</td>\n",
       "      <td>0.689111</td>\n",
       "      <td>0.683287</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.326891</td>\n",
       "      <td>0.747521</td>\n",
       "      <td>No difference with BaseModel with 0.05 confide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  ind  TotalIterations  BestIteration  train-0-fold  \\\n",
       "0    BaseModel    0             1170           1069      0.706434   \n",
       "1  PropertyAge    1             1163           1062      0.704921   \n",
       "\n",
       "   train-1-fold  train-2-fold  train-3-fold  train-4-fold  train-5-fold  ...  \\\n",
       "0      0.709781      0.707839      0.709202       0.70888      0.709031  ...   \n",
       "1      0.708554      0.706692      0.708854       0.70731      0.707417  ...   \n",
       "\n",
       "   valid-6-fold  valid-7-fold  valid-8-fold  valid-9-fold  valid-auc-mean  \\\n",
       "0      0.674756      0.685188      0.693142      0.685688        0.684884   \n",
       "1      0.673745      0.684495      0.692695      0.689111        0.683287   \n",
       "\n",
       "   valid-auc-std  valid-auc-sem  t-statistic    pvalue  \\\n",
       "0       0.010501       0.003321     0.000000  0.000000   \n",
       "1       0.011341       0.003586     0.326891  0.747521   \n",
       "\n",
       "                                             Comment  \n",
       "0                                         Base Model  \n",
       "1  No difference with BaseModel with 0.05 confide...  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t-test for each record in BestResults\n",
    "for index, model in BestResults.iterrows():\n",
    "    if ((model['Model']!=BaseModel) | (model['ind']!=BaseInd)):\n",
    "        AnalyzedModelResults=model[folds_valid_columns].values.tolist()\n",
    "        t=stats.ttest_ind(BaseModelResults,AnalyzedModelResults)\n",
    "        BestResults.at[index,'t-statistic']= t.statistic\n",
    "        BestResults.at[index,'pvalue'] = t.pvalue \n",
    "        if t.pvalue>=confidence_level:\n",
    "            BestResults.at[index,'Comment'] = 'No difference with %s with %s confidence level'%(BaseModel,confidence_level)\n",
    "        else:\n",
    "            BestResults.at[index,'Comment'] = 'There is a difference with %s with %s confidence level'%(BaseModel,confidence_level)\n",
    "BestResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ind</th>\n",
       "      <th>TotalIterations</th>\n",
       "      <th>BestIteration</th>\n",
       "      <th>train-0-fold</th>\n",
       "      <th>train-1-fold</th>\n",
       "      <th>train-2-fold</th>\n",
       "      <th>train-3-fold</th>\n",
       "      <th>train-4-fold</th>\n",
       "      <th>train-5-fold</th>\n",
       "      <th>...</th>\n",
       "      <th>eta</th>\n",
       "      <th>subsample</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>num_round</th>\n",
       "      <th>Training_data</th>\n",
       "      <th>Testing_data</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>1170</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.706434</td>\n",
       "      <td>0.709781</td>\n",
       "      <td>0.707839</td>\n",
       "      <td>0.709202</td>\n",
       "      <td>0.70888</td>\n",
       "      <td>0.709031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/bf2/training/...</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/bf2/testing/B...</td>\n",
       "      <td>yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PropertyAge</td>\n",
       "      <td>1</td>\n",
       "      <td>1163</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>0.708554</td>\n",
       "      <td>0.706692</td>\n",
       "      <td>0.708854</td>\n",
       "      <td>0.70731</td>\n",
       "      <td>0.707417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/bf2/training/...</td>\n",
       "      <td>s3://kdproperty/Data/Experiments/bf2/testing/P...</td>\n",
       "      <td>cal_year-yearbuilt</td>\n",
       "      <td>cova_deductible</td>\n",
       "      <td>sqft</td>\n",
       "      <td>water_risk_3_blk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  ind  TotalIterations  BestIteration  train-0-fold  \\\n",
       "0    BaseModel    0             1170           1069      0.706434   \n",
       "1  PropertyAge    1             1163           1062      0.704921   \n",
       "\n",
       "   train-1-fold  train-2-fold  train-3-fold  train-4-fold  train-5-fold  ...  \\\n",
       "0      0.709781      0.707839      0.709202       0.70888      0.709031  ...   \n",
       "1      0.708554      0.706692      0.708854       0.70731      0.707417  ...   \n",
       "\n",
       "    eta  subsample  max_depth  num_round  \\\n",
       "0  0.02        0.6          3       5000   \n",
       "1  0.02        0.6          3       5000   \n",
       "\n",
       "                                       Training_data  \\\n",
       "0  s3://kdproperty/Data/Experiments/bf2/training/...   \n",
       "1  s3://kdproperty/Data/Experiments/bf2/training/...   \n",
       "\n",
       "                                        Testing_data                  F1  \\\n",
       "0  s3://kdproperty/Data/Experiments/bf2/testing/B...           yearbuilt   \n",
       "1  s3://kdproperty/Data/Experiments/bf2/testing/P...  cal_year-yearbuilt   \n",
       "\n",
       "                F2    F3                F4  \n",
       "0  cova_deductible  sqft  water_risk_3_blk  \n",
       "1  cova_deductible  sqft  water_risk_3_blk  \n",
       "\n",
       "[2 rows x 49 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joining the results of the experiment with the experiment configuration\n",
    "BestResults = pd.concat([BestResults, data_for_training.drop('Model',axis=1)], axis=1)\n",
    "BestResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'ind', 'TotalIterations', 'BestIteration', 'train-0-fold',\n",
       "       'train-1-fold', 'train-2-fold', 'train-3-fold', 'train-4-fold',\n",
       "       'train-5-fold', 'train-6-fold', 'train-7-fold', 'train-8-fold',\n",
       "       'train-9-fold', 'train-auc-mean', 'train-auc-std', 'train-auc-sem',\n",
       "       'valid-0-fold', 'valid-1-fold', 'valid-2-fold', 'valid-3-fold',\n",
       "       'valid-4-fold', 'valid-5-fold', 'valid-6-fold', 'valid-7-fold',\n",
       "       'valid-8-fold', 'valid-9-fold', 'valid-auc-mean', 'valid-auc-std',\n",
       "       'valid-auc-sem', 't-statistic', 'pvalue', 'Comment', 'objective',\n",
       "       'eval_metric', 'booster', 'scale_pos_weight', 'colsample_bylevel',\n",
       "       'colsample_bytree', 'eta', 'subsample', 'max_depth', 'num_round',\n",
       "       'Training_data', 'Testing_data', 'F1', 'F2', 'F3', 'F4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestResults.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ind</th>\n",
       "      <th>TotalIterations</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>valid-auc-mean</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaseModel</td>\n",
       "      <td>0</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.708468</td>\n",
       "      <td>0.684884</td>\n",
       "      <td>Base Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PropertyAge</td>\n",
       "      <td>1</td>\n",
       "      <td>1163</td>\n",
       "      <td>0.707169</td>\n",
       "      <td>0.683287</td>\n",
       "      <td>No difference with BaseModel with 0.05 confide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  ind  TotalIterations  train-auc-mean  valid-auc-mean  \\\n",
       "0    BaseModel    0             1170        0.708468        0.684884   \n",
       "1  PropertyAge    1             1163        0.707169        0.683287   \n",
       "\n",
       "                                             Comment  \n",
       "0                                         Base Model  \n",
       "1  No difference with BaseModel with 0.05 confide...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestResults[['Model','ind','TotalIterations','train-auc-mean','valid-auc-mean','Comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into the Experiment log file models results\n",
    "eu.SaveToExperimentLog(Experiments_file, '%s BestResults'%Experiment_name, BestResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training and validation errors (output from the model) to estimate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for index, row in data_for_training.iterrows():\n",
    "    if len(CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index))])>0:\n",
    "        data=CVResults[( (CVResults['Model']==row['Model']) & (CVResults['ind']==index))]\n",
    "        ax=data[['train-auc-mean','valid-auc-mean']].plot(title=row['Model']+'-'+str(index))\n",
    "        ax.fill_between(data.index.values, (data['train-auc-mean'].values-data['train-auc-sem'].values), (data['train-auc-mean'].values + data['train-auc-sem'].values), color='b', alpha=.1)\n",
    "        ax.fill_between(data.index.values, (data['valid-auc-mean'].values-data['valid-auc-sem'].values), (data['valid-auc-mean'].values + data['valid-auc-sem'].values), color='r', alpha=.1)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into the Experiment log file models results\n",
    "#eu.SaveToExperimentLog(Experiments_file, '%s CVResults'%Experiment_name, CVResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving models artifacts into the Experiment Log file\n",
    "eu.SaveToExperimentLog(Experiments_file, '%s ModelFiles'%Experiment_name, ModelFiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
